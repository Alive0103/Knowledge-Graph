# 文件功能说明

## 数据处理流程文件（按处理顺序）

1. **01_wikidata_add_wikipedia_links.py** (原 wiki_add_pedialink.py)
   - 功能：通过SPARQL查询Wikidata，获取实体的中文和英文维基百科链接
   - 输入：data文件夹中的JSONL文件
   - 输出：data2文件夹，添加了wikipediaLink_zh和wikipediaLink_en字段

2. **02_filter_chinese_wikipedia_data.py** (原 get_data_with_zhlink.py)
   - 功能：筛选出有中文维基百科链接的数据
   - 输入：data2文件夹
   - 输出：data3文件夹，只保留有wikipediaLink_zh的数据

3. **03_wikipedia_download_content.py** (原 wiki_add_content.py)
   - 功能：从维基百科链接下载页面HTML内容，繁体转简体
   - 输入：data3文件夹
   - 输出：data4文件夹，添加了content字段

4. **04_extract_description_from_html.py** (原 description_from_content.py)
   - 功能：从HTML内容中提取第一段作为描述
   - 输入：data3和data4文件夹
   - 输出：更新data3文件夹，添加description字段

5. **05_wikidata_query_aliases.py** (原 wikidata_add_aliases.py)
   - 功能：从Wikidata查询实体的别名（中文和英文），多线程处理
   - 输入：military文件夹中的JSONL文件
   - 输出：new_military文件夹，添加aliases字段

6. **06_translate_aliases_bidirectional.py** (原 wiki_trans_aliases.py)
   - 功能：使用翻译模型（Helsinki-NLP）双向翻译别名，补充中英文别名
   - 输入：data33文件夹
   - 输出：更新原文件，补充zh_aliases和en_aliases字段

7. **07_wikipedia_crawl_recursive.py** (原 wiki_get_other.py)
   - 功能：递归下载维基百科页面内容、图片和链接（深度爬取）
   - 输入：temp/3文件夹
   - 输出：data4/wiki_page.jsonl和data4/wiki_image.jsonl

8. **08_store_to_elasticsearch.py** (原 complete_es.py)
   - 功能：将维基百科内容存储到Elasticsearch（包括页面和图片）
   - 输入：指定路径的JSONL文件
   - 输出：Elasticsearch索引

9. **09_add_description_from_content.py** (原 wikidata_add_description.py)
   - 功能：从HTML内容提取第一段作为描述并添加到数据
   - 输入：data3和data4文件夹
   - 输出：更新data3文件夹

## 工具/展示文件

10. **10_web_display_wikipedia.py** (原 show_page.py)
    - 功能：Flask web应用，用于显示维基百科页面内容（包括图片）
    - 用途：前端展示工具

## 子目录文件说明

### data22/relink/ 目录（实体链接相关处理）

1. **extract_and_deduplicate_links.py** (原 1.py)
   - 功能：从JSONL文件中提取redirect_link字段，去重后保存
   - 输入：当前目录下的所有.jsonl文件
   - 输出：deduplicated_links.jsonl

2. **extract_links_to_single_json.py** (原 2.py)
   - 功能：提取所有redirect_link的值并存入单个JSON文件
   - 输入：deduplicated_links1.jsonl
   - 输出：ent1.jsonl（单个JSON对象格式）

3. **download_wikipedia_content_from_links.py** (原 3.py)
   - 功能：从redirect_link字段抓取维基百科页面HTML内容（多线程）
   - 输入：以"e1"开头的.jsonl文件
   - 输出：原文件名_2.jsonl，包含link、content、images_link字段

4. **chaifen.py**
   - 功能：将大文件拆分成小批次（每批10000条）

5. **hebing.py**
   - 功能：合并当前目录下所有JSONL文件并去重

6. **get_relink_content.py**
   - 功能：与download_wikipedia_content_from_links.py类似，处理以"zh"开头的文件

### data22/ 目录

1. **replace_wiki_links_in_html.py** (原 2.py)
   - 功能：处理HTML内容，将/wiki/开头的超链接替换为完整的维基百科链接
   - 输入：new_data1.jsonl
   - 输出：data1.jsonl

2. **chaifen.py**
   - 功能：将文件拆分成小批次（每批60条）

3. **hebing.py**
   - 功能：合并当前目录下所有JSONL文件并去重

## 已删除的文件

- **test.py** - 已删除（测试版本，功能与08_store_to_elasticsearch.py重复，且有bug）
- **complete_item.py** - 已删除（不完整的代码片段，缺少导入和函数定义）
- **data22/relink/1.py, 2.py, 3.py** - 已重命名
- **data22/2.py** - 已重命名

## 索引
data2.jsonl
顶级键: ['wikipedia', 'wikidata', 'label', 'zh_aliases', 'en_aliases', 'content', 'images', 'redirect_link', 'en_description', 'zh_description', 'zh_descriptions_vector', 'en_descriptions_vector']

data2_relink.jsonl
顶级键: ['content', 'images_link', 'link']

data2_images.jsonl
顶级键: ['link']