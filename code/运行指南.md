# 知识图谱数据处理运行指南

本文档说明如何按顺序运行 `code` 目录下的数据处理脚本。

## 📋 目录结构

```
code/
├── 01_wikidata_add_wikipedia_links.py      # 步骤1: 添加维基百科链接
├── 02_filter_chinese_wikipedia_data.py     # 步骤2: 筛选中文维基数据
├── 03_wikipedia_download_content.py        # 步骤3: 下载维基内容
├── 04_extract_description_from_html.py     # 步骤4: 提取描述
├── 05_wikidata_query_aliases.py            # 步骤5: 查询别名
├── 06_translate_aliases_bidirectional.py    # 步骤6: 翻译别名
├── 07_wikipedia_crawl_recursive.py         # 步骤7: 递归爬取
├── 08_store_to_elasticsearch.py            # 步骤8: 存储到ES
├── 09_add_description_from_content.py      # 步骤9: 添加描述
├── 10_web_display_wikipedia.py            # 步骤10: Web展示
└── data22/                                 # 子目录（实体链接相关）
    ├── relink/                             # 实体链接处理
    └── ruku_demo/                          # 入库演示
```

## 🔧 前置要求

### 1. Python 环境
- Python 3.7 或更高版本
- 推荐使用虚拟环境

### 2. 依赖包安装

```bash
pip install SPARQLWrapper requests opencc beautifulsoup4 transformers torch elasticsearch flask jsonlines tqdm
```

### 3. 模型文件（可选，用于翻译功能）
- 如果使用 `06_translate_aliases_bidirectional.py`，需要下载翻译模型：
  - `Helsinki-NLP-opus-mt-zh-en` (中文到英文)
  - `Helsinki-NLP-opus-mt-en-zh` (英文到中文)
- 将模型放在 `model/` 目录下

### 4. 网络配置
- 部分脚本需要访问维基百科API，可能需要配置代理
- 检查脚本中的代理设置（如 `127.0.0.1:7890`）

### 5. Elasticsearch（可选，用于步骤8）
- 如果使用 `08_store_to_elasticsearch.py`，需要配置ES连接
- 修改脚本中的ES连接信息

## 📊 数据处理流程

### 主流程（按顺序执行）

#### 步骤 1: 添加维基百科链接
```bash
cd code
python 01_wikidata_add_wikipedia_links.py
```
**功能**: 通过SPARQL查询Wikidata，获取实体的中文和英文维基百科链接  
**输入**: `data/` 文件夹中的JSONL文件  
**输出**: `data2/` 文件夹，添加了 `wikipediaLink_zh` 和 `wikipediaLink_en` 字段  
**说明**: 
- 需要网络连接访问Wikidata SPARQL端点
- 使用多线程处理（64线程），可能需要较长时间

#### 步骤 2: 筛选中文维基数据
```bash
python 02_filter_chinese_wikipedia_data.py
```
**功能**: 筛选出有中文维基百科链接的数据  
**输入**: `data2/` 文件夹  
**输出**: `data3/` 文件夹，只保留有 `wikipediaLink_zh` 的数据  
**说明**: 快速筛选，无需网络连接

#### 步骤 3: 下载维基内容
```bash
python 03_wikipedia_download_content.py
```
**功能**: 从维基百科链接下载页面HTML内容，繁体转简体  
**输入**: `data3/` 文件夹  
**输出**: `data4/` 文件夹，添加了 `content` 字段  
**说明**: 
- 需要网络连接访问维基百科API
- 使用代理（127.0.0.1:7890），请根据实际情况修改
- 多线程下载（10线程）

#### 步骤 4: 提取描述
```bash
python 04_extract_description_from_html.py
```
**功能**: 从HTML内容中提取第一段作为描述  
**输入**: `data3/` 和 `data4/` 文件夹  
**输出**: 更新 `data3/` 文件夹，添加 `description` 字段  
**说明**: 本地处理，无需网络连接

#### 步骤 5: 查询别名（可选）
```bash
python 05_wikidata_query_aliases.py
```
**功能**: 从Wikidata查询实体的别名（中文和英文）  
**输入**: `military/` 文件夹中的JSONL文件  
**输出**: `new_military/` 文件夹，添加 `aliases` 字段  
**说明**: 
- 需要网络连接
- 多线程处理（64线程）
- 需要修改脚本中的输入文件夹路径

#### 步骤 6: 翻译别名（可选）
```bash
python 06_translate_aliases_bidirectional.py
```
**功能**: 使用翻译模型双向翻译别名，补充中英文别名  
**输入**: `data33/` 文件夹  
**输出**: 更新原文件，补充 `zh_aliases` 和 `en_aliases` 字段  
**说明**: 
- 需要GPU支持（CUDA）
- 需要下载翻译模型到 `model/` 目录
- 修改脚本中的 `data33_folder` 和 `model_path` 变量

#### 步骤 7: 递归爬取（可选）
```bash
python 07_wikipedia_crawl_recursive.py
```
**功能**: 递归下载维基百科页面内容、图片和链接（深度爬取）  
**输入**: `temp/3/` 文件夹  
**输出**: `data4/wiki_page.jsonl` 和 `data4/wiki_image.jsonl`  
**说明**: 
- 需要网络连接
- 会递归下载链接页面，可能耗时较长
- 需要修改脚本中的输入路径

#### 步骤 8: 存储到Elasticsearch（可选）
```bash
python 08_store_to_elasticsearch.py
```
**功能**: 将维基百科内容存储到Elasticsearch  
**输入**: 指定路径的JSONL文件  
**输出**: Elasticsearch索引  
**说明**: 
- 需要配置Elasticsearch连接信息
- 修改脚本中的ES连接参数和输入路径

#### 步骤 9: 添加描述（可选）
```bash
python 09_add_description_from_content.py
```
**功能**: 从HTML内容提取第一段作为描述并添加到数据  
**输入**: `data3/` 和 `data4/` 文件夹  
**输出**: 更新 `data3/` 文件夹  
**说明**: 本地处理，无需网络连接

#### 步骤 10: Web展示（可选）
```bash
python 10_web_display_wikipedia.py
```
**功能**: Flask web应用，用于显示维基百科页面内容  
**访问**: 浏览器打开 `http://localhost:5000/page/<链接>`  
**说明**: 
- 需要Flask依赖
- 确保 `data4/` 目录下有相应的数据文件

## 🔄 子目录处理流程

### data22/relink/ 目录（实体链接相关）

#### 1. 提取并去重链接
```bash
cd code/data22/relink
python extract_and_deduplicate_links.py
```
**功能**: 从JSONL文件中提取 `redirect_link` 字段，去重后保存  
**输入**: 当前目录下的所有.jsonl文件  
**输出**: `deduplicated_links.jsonl`

#### 2. 提取链接到单个JSON
```bash
python extract_links_to_single_json.py
```
**功能**: 提取所有redirect_link的值并存入单个JSON文件  
**输入**: `deduplicated_links1.jsonl`  
**输出**: `ent1.jsonl`（单个JSON对象格式）

#### 3. 下载维基内容
```bash
python download_wikipedia_content_from_links.py
```
**功能**: 从redirect_link字段抓取维基百科页面HTML内容  
**输入**: 以"e1"开头的.jsonl文件  
**输出**: 原文件名_2.jsonl

#### 4. 文件拆分
```bash
python chaifen.py
```
**功能**: 将大文件拆分成小批次（每批10000条）

#### 5. 文件合并
```bash
python hebing.py
```
**功能**: 合并当前目录下所有JSONL文件并去重

### data22/ 目录

#### 1. 替换HTML中的链接
```bash
cd code/data22
python replace_wiki_links_in_html.py
```
**功能**: 处理HTML内容，将/wiki/开头的超链接替换为完整的维基百科链接  
**输入**: `new_data1.jsonl`  
**输出**: `data1.jsonl`

## 🚀 快速运行方式

### 方式一：完整流程（推荐）

创建一个批处理脚本 `run_all.bat` (Windows) 或 `run_all.sh` (Linux/Mac):

**Windows (run_all.bat):**
```batch
@echo off
chcp 65001 >nul
echo ========================================
echo 知识图谱数据处理流程
echo ========================================
echo.

cd code

echo [步骤 1/4] 添加维基百科链接...
python 01_wikidata_add_wikipedia_links.py
if %errorlevel% neq 0 (
    echo 步骤1失败，请检查错误信息
    pause
    exit /b 1
)

echo [步骤 2/4] 筛选中文维基数据...
python 02_filter_chinese_wikipedia_data.py
if %errorlevel% neq 0 (
    echo 步骤2失败，请检查错误信息
    pause
    exit /b 1
)

echo [步骤 3/4] 下载维基内容...
python 03_wikipedia_download_content.py
if %errorlevel% neq 0 (
    echo 步骤3失败，请检查错误信息
    pause
    exit /b 1
)

echo [步骤 4/4] 提取描述...
python 04_extract_description_from_html.py
if %errorlevel% neq 0 (
    echo 步骤4失败，请检查错误信息
    pause
    exit /b 1
)

echo ========================================
echo 所有步骤完成！
echo ========================================
pause
```

**Linux/Mac (run_all.sh):**
```bash
#!/bin/bash
echo "========================================"
echo "知识图谱数据处理流程"
echo "========================================"
echo ""

cd code

echo "[步骤 1/4] 添加维基百科链接..."
python 01_wikidata_add_wikipedia_links.py
if [ $? -ne 0 ]; then
    echo "步骤1失败，请检查错误信息"
    exit 1
fi

echo "[步骤 2/4] 筛选中文维基数据..."
python 02_filter_chinese_wikipedia_data.py
if [ $? -ne 0 ]; then
    echo "步骤2失败，请检查错误信息"
    exit 1
fi

echo "[步骤 3/4] 下载维基内容..."
python 03_wikipedia_download_content.py
if [ $? -ne 0 ]; then
    echo "步骤3失败，请检查错误信息"
    exit 1
fi

echo "[步骤 4/4] 提取描述..."
python 04_extract_description_from_html.py
if [ $? -ne 0 ]; then
    echo "步骤4失败，请检查错误信息"
    exit 1
fi

echo "========================================"
echo "所有步骤完成！"
echo "========================================"
```

### 方式二：手动运行

按照上述步骤顺序，逐个运行脚本。

## ⚠️ 注意事项

1. **网络连接**: 步骤1、3、5、7需要网络连接访问Wikidata和维基百科API
2. **代理配置**: 如果在中国大陆，可能需要配置代理，检查脚本中的代理设置
3. **文件路径**: 确保输入文件夹存在且包含数据文件
4. **GPU支持**: 步骤6需要GPU支持，如果没有GPU可以跳过或使用CPU版本
5. **Elasticsearch**: 步骤8需要配置ES连接，确保ES服务正常运行
6. **数据量**: 处理大量数据时，可能需要较长时间，请耐心等待
7. **错误处理**: 如果某个步骤失败，检查错误信息并修复后重新运行

## 📝 数据文件格式说明

### 输入数据格式（data/）
每行一个JSON对象，包含：
- `item`: 实体信息
- 其他字段...

### 输出数据格式（data2/）
- `wikipediaLink_zh`: 中文维基百科链接
- `wikipediaLink_en`: 英文维基百科链接

### 输出数据格式（data3/）
- `wikipediaLink_zh`: 中文维基百科链接（必须有）
- `description`: 描述（步骤4后添加）

### 输出数据格式（data4/）
- `label`: 实体标签
- `link`: 维基百科链接
- `content`: HTML内容

## 🔍 故障排查

### 问题1: 网络连接失败
- 检查网络连接
- 检查代理设置是否正确
- 尝试直接访问维基百科网站

### 问题2: 文件不存在错误
- 检查输入文件夹是否存在
- 检查文件夹中是否有.jsonl文件
- 确认文件路径正确

### 问题3: 内存不足
- 减少线程数（修改 `max_workers` 参数）
- 分批处理数据
- 增加系统内存

### 问题4: 翻译模型加载失败
- 检查模型文件是否存在
- 确认模型路径正确
- 检查是否有GPU支持

## 📞 获取帮助

如果遇到问题，请：
1. 检查错误日志
2. 查看文件功能说明文档
3. 确认所有依赖已正确安装
4. 检查配置文件是否正确

