# 检索模式评测说明文档

## 📋 目录

- [概述](#概述)
- [七种检索模式详解](#七种检索模式详解)
- [使用方法](#使用方法)
- [评测结果分析](#评测结果分析)
- [技术实现细节](#技术实现细节)

---

## 概述

本系统实现了七种不同的检索模式，用于实体链接任务。每种模式结合了不同的检索策略和排序方法，适用于不同的应用场景。

### 七种检索模式

1. **文本检索+LLM** (`text`): 基于关键词匹配 + LLM 重排序
2. **向量检索+LLM** (`vector`): 基于语义向量检索 + LLM 重排序
3. **向量检索(无LLM)** (`vector_no_llm`): 纯向量检索，按相似度排序
4. **向量检索(对齐)** (`vector_aligned`): 使用 LLM 生成定义后向量化，对齐查询和索引向量
5. **向量检索(双字段)** (`vector_dual`): 同时搜索描述向量和标签向量并融合
6. **混合检索(无LLM)** (`hybrid_no_llm`): 文本+向量混合，仅 ES 排序
7. **混合检索+LLM** (`hybrid`): 文本+向量混合 + LLM 重排序（推荐）

### 评测指标说明

- **MRR (Mean Reciprocal Rank)**: 平均倒数排名，衡量正确结果的平均排名质量
- **Hit@1**: Top-1 准确率，正确结果排在第一位的比例
- **Hit@5**: Top-5 准确率，正确结果排在前5位的比例
- **Hit@10**: Top-10 准确率，正确结果排在前10位的比例

---

## 七种检索模式详解

### 1. Text 模式（文本检索 + LLM 重排序）

**模式标识**: `text`

**检索策略**:
- 使用 Elasticsearch 的文本匹配检索
- 在 `label` 和 `aliases_zh` 字段上进行关键词匹配
- 使用布尔查询（bool query）的 should 子句，boost 权重为 1.0

**排序方法**:
- 使用 LLM（智谱AI GLM-4-Flash）进行重排序
- LLM 根据输入提及、别名、定义与候选实体的匹配度进行排序

**适用场景**:
- 查询文本与实体名称高度匹配的情况
- 需要精确匹配的场景
- 对语义理解要求不高的场景

**代码实现**:
```python
# 检索
results = text_search(query_text, top_k=20)

# 重排序
sorted_links = generate_prompt_and_sort(query, results)
```

---

### 2. Vector 模式（向量检索 + LLM 重排序）

**模式标识**: `vector`

**检索策略**:
- 使用语义向量检索（KNN 搜索）
- 对查询文本生成 1024 维向量（使用 Chinese-RoBERTa-Large 模型）
- 在 `descriptions_zh_vector` 和 `descriptions_en_vector` 字段上进行 KNN 搜索
- 融合中英文向量检索结果，保留最高分数

**排序方法**:
- 使用 LLM 进行重排序（与 Text 模式相同）

**适用场景**:
- 查询文本与实体名称不完全匹配，但语义相近的情况
- 需要语义理解能力的场景
- 跨语言检索场景

**代码实现**:
```python
# 检索（支持中英向量融合）
results = vector_search(query_text, top_k=20)

# 重排序
sorted_links = generate_prompt_and_sort(query, results)
```

---

### 3. Hybrid_No_LLM 模式（混合检索，仅 ES 排序）

**模式标识**: `hybrid_no_llm`

**检索策略**:
- **文本检索**: 在 `label` 和 `aliases_zh` 字段上进行关键词匹配
- **向量检索**: 使用查询文本直接生成向量（不通过 LLM 生成定义），在向量字段上进行 KNN 搜索
- 将文本检索和向量检索结果进行混合，使用 ES 的混合查询（hybrid query）

**排序方法**:
- **不使用 LLM 重排序**
- 直接按照 Elasticsearch 返回的 `_score` 排序
- 这是纯 ES 混合检索的 baseline

**适用场景**:
- 需要快速响应的场景（无 LLM 调用延迟）
- 评估 ES 混合检索本身的效果
- 作为 ablation study 的对照组

**代码实现**:
```python
# 检索（混合，但不使用 LLM 生成定义）
results = hybrid_search(query_text, use_vector=True, use_llm_definition=False)

# 直接按 ES 分数排序，不调用 LLM
sorted_links = [r.get("link", "") for r in results]
```

---

### 4. Hybrid 模式（混合检索 + LLM 重排序）

**模式标识**: `hybrid`

**检索策略**:
- **文本检索**: 在 `label` 和 `aliases_zh` 字段上进行关键词匹配
- **向量检索**: 
  - 使用 LLM 生成规范化定义（`get_alias_and_definition`）
  - 对定义文本生成向量
  - 在向量字段上进行 KNN 搜索
- 将文本检索和向量检索结果进行混合

**排序方法**:
- 使用 LLM 进行重排序（与 Text/Vector 模式相同）

**适用场景**:
- **推荐使用**：综合了文本匹配和语义理解的优势
- 需要高准确率的场景
- 查询文本可能不完整或不规范的情况

**代码实现**:
```python
# 检索（混合，使用 LLM 生成定义）
results = hybrid_search(query_text, use_vector=True, use_llm_definition=True)

# 重排序
sorted_links = generate_prompt_and_sort(query, results)
```

---

### 5. Vector_No_LLM 模式（纯向量检索，无 LLM 重排序）

**模式标识**: `vector_no_llm`

**检索策略**:
- 使用语义向量检索（KNN 搜索）
- 对查询文本直接生成 1024 维向量（使用 Chinese-RoBERTa-Large 模型）
- 在 `descriptions_zh_vector` 和 `descriptions_en_vector` 字段上进行 KNN 搜索
- 融合中英文向量检索结果，保留最高分数

**排序方法**:
- **不使用 LLM 重排序**
- 直接按照 Elasticsearch 返回的相似度分数排序（余弦相似度）

**适用场景**:
- 评估纯向量检索的 baseline 效果
- 需要快速响应的场景（无 LLM 调用延迟）
- 作为 ablation study 的对照组

**代码实现**:
```python
# 检索（纯向量，无 LLM）
results = vector_search(query_text, top_k=20)

# 直接按 ES 相似度分数排序
sorted_links = [r.get("link", "") for r in results]
```

---

### 6. Vector_Aligned 模式（对齐查询向量和索引向量）

**模式标识**: `vector_aligned`

**检索策略**:
- **方案A：对齐方案**
- 使用 LLM 生成规范化定义（`get_alias_and_definition`）
- 对定义文本生成向量（而非直接对查询文本生成向量）
- 在 `descriptions_zh_vector` 和 `descriptions_en_vector` 字段上进行 KNN 搜索
- 目标：对齐查询向量（从定义生成）和索引向量（从描述生成）的语义空间

**排序方法**:
- **不使用 LLM 重排序**
- 直接按照 Elasticsearch 返回的相似度分数排序

**适用场景**:
- 尝试解决查询向量（短文本）和索引向量（长描述）的语义空间不匹配问题
- 评估对齐方案的效果

**代码实现**:
```python
# 检索（使用 LLM 生成定义后向量化）
results = vector_search(query_text, use_llm_definition=True, top_k=20)

# 直接按 ES 相似度分数排序
sorted_links = [r.get("link", "") for r in results]
```

**效果**: MRR 0.1488，比纯向量检索（0.0587）提升 **153%**，说明对齐方案有效。

---

### 7. Vector_Dual 模式（双字段向量检索）

**模式标识**: `vector_dual`

**检索策略**:
- **方案B：双字段方案**
- 对查询文本生成向量
- **同时搜索四个向量字段**：
  - `descriptions_zh_vector`（中文描述向量）
  - `descriptions_en_vector`（英文描述向量）
  - `label_zh_vector`（中文标签+别名向量）
  - `label_en_vector`（英文标签+别名向量）
- 融合所有字段的检索结果，保留最高分数
- 目标：为短文本查询提供更好的匹配（通过标签向量）

**排序方法**:
- **不使用 LLM 重排序**
- 直接按照 Elasticsearch 返回的相似度分数排序

**适用场景**:
- 尝试解决短文本查询与长描述向量的匹配问题
- 评估双字段检索的效果

**代码实现**:
```python
# 检索（同时搜索描述向量和标签向量）
results = vector_search(query_text, use_label_vector=True, top_k=20)

# 直接按 ES 相似度分数排序
sorted_links = [r.get("link", "") for r in results]
```

**效果**: MRR 0.1550，比纯向量检索（0.0587）提升 **164%**，比对齐方案（0.1488）略好。

---

## 使用方法

### 基本使用

```bash
cd 实体链接

# 默认：评测所有四种模式并输出对比
python search_withllm.py

# 或显式指定
python search_withllm.py --all
```

### 单独评测某种模式

```bash
# 只评测文本检索模式
python search_withllm.py --text

# 只评测向量检索模式
python search_withllm.py --vector

# 只评测混合检索（无 LLM 重排序）
python search_withllm.py --hybrid-no-llm

# 只评测混合检索（含 LLM 重排序）
python search_withllm.py --hybrid

# 只评测向量检索（无 LLM 重排序）
python search_withllm.py --vector-no-llm

# 只评测向量检索（对齐方案）
python search_withllm.py --vector-aligned

# 只评测向量检索（双字段方案）
python search_withllm.py --vector-dual
```

### 测试功能（无需评测文件）

```bash
# 测试向量检索功能
python search_withllm.py --test-vector

# 测试所有检索模式
python search_withllm.py --test-modes
```

### 输出文件

评测完成后会生成以下文件：

1. **控制台输出**: 实时显示每种模式的评测进度和结果
2. **详细报告**: `evaluation_report_{mode}_{timestamp}.json`
   - 包含每个查询的详细结果、排名、指标
3. **汇总报告**: `evaluation_summary_{timestamp}.json`
   - 包含所有模式的汇总对比

---

## 评测结果分析

### 最新评测结果（2025-12-02）

| 模式 | MRR | Hit@1 | Hit@5 | Hit@10 | 排名 |
|------|-----|-------|-------|--------|------|
| **混合检索+LLM** | **0.7191** | **0.6644** | **0.7860** | **0.8086** | 🥇 1 |
| **文本检索+LLM** | 0.7145 | 0.6644 | 0.7748 | 0.7995 | 🥈 2 |
| **混合检索(无LLM)** | 0.6263 | 0.5428 | 0.7342 | 0.7815 | 🥉 3 |
| **向量检索(双字段)** | 0.1550 | 0.1149 | 0.1914 | 0.2545 | 4 |
| **向量检索(对齐)** | 0.1488 | 0.0878 | 0.2185 | 0.2703 | 5 |
| **向量检索+LLM** | 0.1048 | 0.0856 | 0.1261 | 0.1284 | 6 |
| **向量检索(无LLM)** | 0.0587 | 0.0383 | 0.0788 | 0.1126 | 7 |

### 结果分析

#### 1. 混合检索+LLM 表现最佳（MRR: 0.7191）🏆

**原因分析**:
- ✅ **综合优势**: 结合了文本匹配和语义理解，兼顾精确性和召回率
- ✅ **LLM 增强**: 使用 LLM 生成规范化定义，提升向量检索质量
- ✅ **LLM 重排序**: 最终排序阶段 LLM 的精细判断
- ✅ **召回能力强**: 在 Hit@5 (0.7860) 和 Hit@10 (0.8086) 上表现最优

**结论**: 
> **混合检索+LLM 是综合性能最优的模式，适合大多数生产场景。**

---

#### 2. 文本检索+LLM 表现优秀（MRR: 0.7145）

**原因分析**:
- ✅ **综合优势**: 结合了文本匹配和语义理解
- ✅ **LLM 增强**: 使用 LLM 生成规范化定义，提升向量检索质量
- ✅ **LLM 重排序**: 最终排序阶段 LLM 的精细判断
- ⚠️ **略低于 Text**: 可能因为向量检索引入了一些噪声，或 LLM 生成的定义不够精确

**结论**: Hybrid 模式在 Top-5 和 Top-10 上略优于 Text，说明在候选集较大时，混合检索能提供更好的召回。

---

#### 3. Hybrid_No_LLM 模式（MRR: 0.6246）

**原因分析**:
- ✅ **ES 混合检索有效**: 比纯向量检索好很多，说明文本+向量的混合策略有效
- ⚠️ **缺少 LLM 重排序**: 仅靠 ES 分数排序，无法进行细粒度的语义判断
- ⚠️ **性能差距**: 与 Hybrid 模式相比，MRR 下降约 15%，Hit@1 下降约 22%

**结论**: 
- LLM 重排序带来了**显著的性能提升**（+15% MRR）
- ES 混合检索本身已经比纯向量检索好很多（0.6263 vs 0.0587，提升 **10.7 倍**）

---

#### 4. 向量检索优化方案对比

**向量检索(双字段) (MRR: 0.1550)** vs **向量检索(对齐) (MRR: 0.1488)** vs **向量检索(无LLM) (MRR: 0.0587)**

**关键发现**:
- ✅ **双字段方案最优**: 同时搜索描述向量和标签向量，MRR 达到 0.1550
- ✅ **对齐方案有效**: 使用 LLM 生成定义后向量化，MRR 达到 0.1488
- ✅ **优化方案显著提升**: 双字段方案比纯向量检索提升 **164%**，对齐方案提升 **153%**
- ⚠️ **但仍远低于文本/混合检索**: 即使优化后，向量检索 MRR 仍只有 0.15，远低于文本检索的 0.71

**结论**: 
> **向量检索优化方案（对齐、双字段）虽然显著提升了纯向量检索的效果，但仍无法与文本检索或混合检索相比。向量检索更适合作为混合检索的补充，而非独立使用。**

---

#### 5. 向量检索模式表现较差（MRR: 0.0587-0.1048）

**原因分析**:
- ❌ **语义空间拥挤**: 军事实体（如各种型号的步枪、坦克、战斗机）在语义空间中非常接近
- ❌ **缺乏精确匹配**: 纯向量检索无法利用实体名称的精确匹配信息
- ❌ **描述文本相似**: 很多实体的描述文本在语义上高度相似，难以区分
- ❌ **查询-索引向量不匹配**: 查询向量（短文本）和索引向量（长描述）的语义空间不一致

**排序机制**:
- 在不使用 LLM 的情况下，向量检索的排序完全依赖于 Elasticsearch 的 KNN 搜索
- 使用余弦相似度计算查询向量与文档向量之间的相似度
- 根据相似度得分从高到低排序，得分越高表示越相似

**结论**: 
- 纯向量检索不适合**细粒度实体区分**任务
- 即使使用对齐方案或双字段方案优化，效果仍远低于文本/混合检索
- 向量检索更适合作为混合检索的补充，提供语义召回能力

---

### 关键发现

1. **混合检索+LLM 综合最优**
   - Hybrid (0.7191) > Text (0.7145): 在 Hit@5 和 Hit@10 上表现更好
   - 说明混合检索在需要更大候选集时更有优势

2. **LLM 重排序的重要性**
   - Hybrid_No_LLM (0.6263) vs Hybrid (0.7191): **+14.8% MRR 提升**
   - 说明 LLM 的细粒度判断能力对最终排序至关重要

3. **向量检索优化方案的有效性**
   - 向量检索(双字段) (0.1550) vs 向量检索(无LLM) (0.0587): **+164% 提升**
   - 向量检索(对齐) (0.1488) vs 向量检索(无LLM) (0.0587): **+153% 提升**
   - 说明对齐方案和双字段方案都能显著提升向量检索效果

4. **向量检索的局限性**
   - 即使优化后，向量检索 MRR 仍只有 0.15，远低于文本检索的 0.71
   - 向量检索更适合作为混合检索的补充，而非独立使用

5. **文本匹配的优势**
   - Text 模式 (0.7145) 在 Hit@1 上与 Hybrid 持平
   - 在实体名称规范、查询匹配度高的场景下，文本检索已经足够好

---

## 技术实现细节

### 向量生成优化

1. **GPU 加速**: 自动检测并使用 GPU（NVIDIA CUDA），大幅提升向量生成速度
2. **批量处理**: 在评测模式下，预先批量生成所有查询的向量，避免重复计算
3. **向量缓存**: 使用 LRU 缓存机制，缓存最近生成的向量

### 中英向量融合

`vector_search()` 函数实现了中英向量字段的融合检索：

```python
# 对 descriptions_zh_vector 和 descriptions_en_vector 分别进行 KNN 搜索
# 融合结果，保留最高分数
```

### LLM 重排序机制

1. **规范化定义生成**: 使用 LLM 将查询文本转换为规范化定义
2. **候选实体排序**: LLM 根据输入信息和候选实体的匹配度进行排序
3. **链接匹配**: 支持多种链接匹配方式（精确匹配、URL 归一化、模糊匹配）

### 性能优化

- **批量向量生成**: 评测时预先批量生成所有查询向量
- **并行处理**: 使用 ThreadPoolExecutor 并行处理多个查询
- **ES 连接复用**: 复用 ES 客户端连接，减少连接开销

---

## 推荐使用策略

### 生产环境推荐

1. **首选**: `hybrid` 模式（混合检索+LLM）
   - 综合性能最好（MRR: 0.7191）
   - 在 Hit@5 和 Hit@10 上表现最优
   - 适合大多数场景

2. **备选**: `text` 模式（文本检索+LLM）
   - 如果查询文本与实体名称高度匹配
   - 需要最快响应速度（无向量生成延迟）
   - 性能与 Hybrid 接近（MRR: 0.7145）

3. **不推荐**: 纯向量检索模式
   - 即使使用对齐方案或双字段方案优化，效果仍远低于文本/混合检索
   - 向量检索更适合作为混合检索的补充

### 研究/实验场景

1. **Ablation Study**: 使用 `hybrid_no_llm` 作为对照组，评估 LLM 重排序的贡献
2. **向量检索研究**: 使用 `vector` 模式评估纯语义检索的效果
3. **综合对比**: 使用 `--all` 参数进行完整对比

---

## 常见问题

### Q1: 为什么 Vector 模式表现这么差？

**A**: 军事实体在语义空间中非常接近，纯向量检索难以区分。需要结合文本匹配或 LLM 的领域知识。

### Q2: Text 模式为什么比 Hybrid 模式还好？

**A**: 在实体名称规范、查询匹配度高的场景下，文本匹配已经足够精确。Hybrid 模式在 Top-5/Top-10 上略优，说明在更大候选集时更有优势。

### Q3: LLM 重排序真的有必要吗？

**A**: 是的。Hybrid_No_LLM (0.6246) vs Hybrid (0.7073) 的对比显示，LLM 重排序带来了 13% 的 MRR 提升。

### Q4: 如何进一步提升性能？

**A**: 
- 优化 LLM prompt，提升定义生成质量
- 调整文本/向量检索的 boost 权重
- 增加更多训练数据，提升向量模型质量
- 优化 ES 索引配置和查询策略

---

## 更新日志

- **2025-12-02**: 添加向量检索优化方案（对齐、双字段），实现完整的七种模式对比
- **2025-12-02**: 更新最新评测结果，混合检索+LLM 表现最优（MRR: 0.7191）
- **2025-12-01**: 添加 Hybrid_No_LLM 模式，实现完整的四种模式对比
- **2025-12-01**: 优化批量向量生成，提升评测速度
- **2025-12-01**: 实现中英向量融合检索

---

## 参考文献

- Elasticsearch KNN Search: https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html
- Chinese-RoBERTa-Large: https://huggingface.co/hfl/chinese-roberta-wwm-ext-large
- 智谱AI GLM-4: https://open.bigmodel.cn/

