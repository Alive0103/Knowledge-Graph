import torch
from transformers import BertTokenizer, BertModel
from zhipuai import ZhipuAI
import numpy as np
import re
from urllib.parse import unquote
from es_client import es
import logging
import pandas as pd
import json
import os
from datetime import datetime

# é…ç½®æ—¥å¿—è®°å½•ï¼ˆè¯¦ç»†æ¨¡å¼ï¼‰
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('vector_search_debug.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = "debug_output"
os.makedirs(output_dir, exist_ok=True)

# æ¨¡å‹åŠ è½½ï¼ˆç”¨äºå‘é‡ç”Ÿæˆï¼Œæ”¯æŒ GPU åŠ é€Ÿï¼‰
model_name = './model/chinese-roberta-wwm-ext-large'
model = None
tokenizer = None
device = torch.device("cpu")
try:
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertModel.from_pretrained(model_name)
    model.eval()

    if torch.cuda.is_available():
        device = torch.device("cuda")
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
        print(f"âœ… æ£€æµ‹åˆ° GPU: {gpu_name} ({gpu_mem:.1f} GB)")
    else:
        device = torch.device("cpu")
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è¿›è¡Œåœ¨çº¿å‘é‡æ£€ç´¢ï¼ˆæ€§èƒ½ä¼šç•¥æ…¢ï¼‰")

    model = model.to(device)
    model_dimension = model.config.hidden_size
    print(f"âœ“ Chinese-RoBERTaæ¨¡å‹åŠ è½½æˆåŠŸ (ç»´åº¦: {model_dimension}, è®¾å¤‡: {device})")
except Exception as e:
    print(f"è­¦å‘Š: æ¨¡å‹åŠ è½½å¤±è´¥ ({e})ï¼Œå‘é‡ç”ŸæˆåŠŸèƒ½å°†ä¸å¯ç”¨")

# æ™ºè°±AI APIå®¢æˆ·ç«¯
client = ZhipuAI(api_key="1a2a485fe1fc4bd5aa0d965bf452c8c8.se8RZdT8cH8skEDo")

# å‘é‡ç¼“å­˜å­—å…¸
_vector_cache = {}
_cache_max_size = 1000


def preprocess_query(query):
    """
    é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬
    - æ¸…ç†æ¢è¡Œç¬¦ã€å¤šä½™ç©ºæ ¼
    - ç»Ÿä¸€æ ¼å¼
    """
    if not query:
        return ""

    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    query = str(query)

    # æ›¿æ¢æ¢è¡Œç¬¦ä¸ºç©ºæ ¼
    query = query.replace('\n', ' ').replace('\r', ' ')

    # æ¸…ç†å¤šä½™ç©ºæ ¼
    query = ' '.join(query.split())

    # æ¸…ç†é¦–å°¾ç©ºæ ¼
    query = query.strip()

    return query


def generate_vector(text, use_cache=True, verbose=True):
    """
    ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆéœ€è¦æ¨¡å‹å·²åŠ è½½ï¼‰

    Args:
        text: è¾“å…¥æ–‡æœ¬
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    # é¢„å¤„ç†æ–‡æœ¬
    text = preprocess_query(text)

    if verbose:
        print(f"\n{'=' * 60}")
        print(f"ğŸ”¹ ç”Ÿæˆå‘é‡: '{text}'")
        print(f"{'=' * 60}")

    if not text or model is None or tokenizer is None:
        if verbose:
            print("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå‘é‡")
        return None

    # ä½¿ç”¨ç¼“å­˜
    if use_cache:
        cache_key = text
        if cache_key in _vector_cache:
            if verbose:
                print(f"âœ… ä½¿ç”¨ç¼“å­˜å‘é‡ï¼ˆå·²å­˜åœ¨ï¼‰")
            return _vector_cache[cache_key]

    if verbose:
        print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆå‘é‡...")

    # ç”Ÿæˆå‘é‡
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
    if verbose:
        print(f"   è¾“å…¥tokenæ•°é‡: {inputs['input_ids'].shape[1]}")

    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    vector = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

    if verbose:
        print(f"   åŸå§‹å‘é‡ç»´åº¦: {len(vector)}")

    # L2å½’ä¸€åŒ–
    norm = np.linalg.norm(vector)
    if norm > 0:
        vector = vector / norm
    if verbose:
        print(f"   L2å½’ä¸€åŒ–åèŒƒæ•°: {np.linalg.norm(vector):.6f}")

    # å¤„ç†ç»´åº¦é—®é¢˜ï¼šESéœ€è¦1024ç»´å‘é‡
    vector_dim = len(vector)
    target_dim = 1024

    if vector_dim == target_dim:
        if verbose:
            print(f"   âœ… å‘é‡ç»´åº¦åŒ¹é…ï¼ˆ{vector_dim}ç»´ï¼‰")
    elif vector_dim < target_dim:
        if verbose:
            print(f"   âš ï¸  å‘é‡ç»´åº¦ä¸è¶³ï¼ˆ{vector_dim}ç»´ï¼‰ï¼Œå¡«å……åˆ°{target_dim}ç»´")
        vector = np.pad(vector, (0, target_dim - vector_dim), 'constant', constant_values=0)
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
    else:
        if verbose:
            print(f"   âš ï¸  å‘é‡ç»´åº¦è¶…å‡ºï¼ˆ{vector_dim}ç»´ï¼‰ï¼Œæˆªæ–­åˆ°{target_dim}ç»´")
        vector = vector[:target_dim]
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm

    vector_list = vector.tolist()

    if verbose:
        print(f"   âœ… æœ€ç»ˆå‘é‡ç»´åº¦: {len(vector_list)}")
        print(f"   âœ… å‘é‡å‰5ä¸ªå€¼: {vector_list[:5]}")

    # æ·»åŠ åˆ°ç¼“å­˜
    if use_cache:
        if len(_vector_cache) >= _cache_max_size:
            first_key = next(iter(_vector_cache))
            del _vector_cache[first_key]
        _vector_cache[cache_key] = vector_list

    return vector_list


def vector_search(query_text, top_k=20, query_vector=None, verbose=True):
    """
    å•ç‹¬çš„å‘é‡æ£€ç´¢ï¼ˆåŒæ—¶æ£€ç´¢æ‰€æœ‰7ä¸ªå‘é‡å­—æ®µï¼‰

    Args:
        query_text: æŸ¥è¯¢æ–‡æœ¬
        top_k: è¿”å›ç»“æœæ•°é‡
        query_vector: é¢„è®¡ç®—çš„æŸ¥è¯¢å‘é‡ï¼ˆå¯é€‰ï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    # é¢„å¤„ç†æŸ¥è¯¢
    query_text = preprocess_query(query_text)

    if verbose:
        print(f"\n{'=' * 60}")
        print(f"ğŸ” å‘é‡æ£€ç´¢: '{query_text}'")
        print(f"{'=' * 60}")

    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    if query_vector is None and model is not None and tokenizer is not None:
        if verbose:
            print(f"ğŸ“ ç”ŸæˆæŸ¥è¯¢å‘é‡...")
        try:
            query_vector = generate_vector(query_text, use_cache=True, verbose=verbose)
        except Exception as e:
            if verbose:
                print(f"âŒ å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            query_vector = None

    if query_vector is None:
        if verbose:
            print(f"âŒ æ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œè¿”å›ç©ºç»“æœ")
        return []

    if verbose:
        print(f"âœ… æŸ¥è¯¢å‘é‡å·²å‡†å¤‡ï¼ˆç»´åº¦: {len(query_vector)}ï¼‰")

    # ç¡®å®šè¦æœç´¢çš„å‘é‡å­—æ®µï¼ˆåŒæ—¶æ£€ç´¢æ‰€æœ‰å‘é‡å­—æ®µï¼‰
    vector_fields = [
        ("descriptions_zh_vector", "zh", "desc"),
        ("descriptions_en_vector", "en", "desc"),
        ("high_freq_words_zh_vector", "zh", "high_freq"),
        ("high_freq_words_en_vector", "en", "high_freq"),
        ("label_vector", "mixed", "label"),
        ("label_zh_vector", "zh", "label"),
        ("label_en_vector", "en", "label")
    ]

    if verbose:
        print(f"\nğŸ“‹ æœç´¢æ‰€æœ‰å‘é‡å­—æ®µï¼ˆå…±{len(vector_fields)}ä¸ªï¼‰:")
        for field_name, lang_tag, field_type in vector_fields:
            print(f"   - {field_name} ({lang_tag}, {field_type})")

    # åŒæ—¶å¯¹å¤šä¸ªå‘é‡å­—æ®µåšæ£€ç´¢ï¼Œç„¶åèåˆç»“æœ
    index_names = ["data2"]  # åªä½¿ç”¨data2ç´¢å¼•
    merged_hits = {}

    for index_name in index_names:
        if verbose:
            print(f"\nğŸ” å°è¯•ç´¢å¼•: {index_name}")
        try:
            if not es.indices.exists(index=index_name):
                if verbose:
                    print(f"   âš ï¸  ç´¢å¼•ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            if verbose:
                print(f"   âœ… ç´¢å¼•å­˜åœ¨")
        except Exception as e:
            if verbose:
                print(f"   âŒ æ£€æŸ¥ç´¢å¼•å¤±è´¥: {e}")
            continue

        # é’ˆå¯¹å½“å‰ç´¢å¼•ï¼Œå°è¯•æ‰€æœ‰å‘é‡å­—æ®µ
        for field_name, lang_tag, field_type in vector_fields:
            if verbose:
                print(f"\n   ğŸ” æœç´¢å­—æ®µ: {field_name} (è¯­è¨€: {lang_tag}, ç±»å‹: {field_type})")

            knn_query = {
                "field": field_name,
                "query_vector": query_vector,
                "k": top_k * 2,  # å¢åŠ å€™é€‰æ•°é‡
                "num_candidates": top_k * 5  # å¢åŠ å€™é€‰æ•°é‡
            }
            search_body = {
                "knn": knn_query,
                "size": top_k * 2  # å¢åŠ è¿”å›æ•°é‡
            }
            try:
                resp = es.search(index=index_name, body=search_body)
                hits = resp.get("hits", {}).get("hits", []) or []

                if verbose:
                    print(f"      âœ… æ‰¾åˆ° {len(hits)} ä¸ªç»“æœ")

                for hit in hits:
                    doc_id = hit.get("_id")
                    if not doc_id:
                        continue
                    key = (index_name, doc_id)
                    score = float(hit.get("_score", 0.0) or 0.0)

                    if verbose and len(merged_hits) < 3:  # åªæ‰“å°å‰3ä¸ª
                        source = hit.get("_source", {})
                        label = source.get("label", "N/A")
                        print(f"         - æ–‡æ¡£ID: {doc_id}, åˆ†æ•°: {score:.4f}, æ ‡ç­¾: {label}")

                    # å¦‚æœåŒä¸€ä¸ªæ–‡æ¡£è¢«å¤šä¸ªå­—æ®µå‘½ä¸­ï¼Œä¿ç•™æœ€é«˜åˆ†
                    if key not in merged_hits or score > merged_hits[key]["score"]:
                        merged_hits[key] = {
                            "source": hit.get("_source", {}),
                            "score": score,
                            "lang": lang_tag,
                            "field_type": field_type
                        }
            except Exception as e:
                if verbose:
                    print(f"      âŒ æœç´¢å¤±è´¥: {e}")
                continue

    if not merged_hits:
        if verbose:
            print(f"\nâŒ æœªæ‰¾åˆ°ä»»ä½•ç»“æœ")
        return []

    if verbose:
        print(f"\nâœ… åˆå¹¶åå…±æ‰¾åˆ° {len(merged_hits)} ä¸ªå”¯ä¸€æ–‡æ¡£")

    # æŒ‰å¾—åˆ†æ’åºï¼Œå–å‰ top_k
    sorted_items = sorted(merged_hits.values(), key=lambda x: x["score"], reverse=True)[:top_k]
    results = []
    for item in sorted_items:
        source = item["source"]
        result = {
            "label": source.get("label", ""),
            "aliases_zh": source.get("aliases_zh", []),
            "aliases_en": source.get("aliases_en", []),
            "descriptions_zh": source.get("descriptions_zh", ""),
            "descriptions_en": source.get("descriptions_en", ""),
            "link": source.get("link", ""),
            "_score": item["score"],
            "_lang": item["lang"],
            "_field_type": item["field_type"]
        }
        results.append(result)

    if verbose:
        print(f"\nğŸ“Š å‘é‡æ£€ç´¢ç»“æœï¼ˆå‰{min(5, len(results))}ä¸ªï¼‰:")
        for i, result in enumerate(results[:5], 1):
            print(f"   {i}. {result.get('label', 'N/A')}")
            print(f"      åˆ†æ•°: {result.get('_score', 0):.4f}")
            print(f"      é“¾æ¥: {result.get('link', 'N/A')}")
            print(f"      æè¿°: {result.get('descriptions_zh', '')[:80]}...")
            print()

    return results


def get_alias_and_definition(mention, verbose=True):
    """è·å–å®ä½“çš„åˆ«åã€å®šä¹‰å’Œè¯¦ç»†æè¿°ï¼ˆä¸­è‹±æ–‡å„ä¸€ç‰ˆï¼‰"""
    # é¢„å¤„ç†æŸ¥è¯¢
    mention = preprocess_query(mention)

    if verbose:
        print(f"\n{'=' * 60}")
        print(f"ğŸ¤– LLMè°ƒç”¨: è·å–åˆ«åã€å®šä¹‰å’Œè¯¦ç»†æè¿°ï¼ˆä¸­è‹±æ–‡ï¼‰")
        print(f"{'=' * 60}")
        print(f"ğŸ“ è¾“å…¥æåŠ: '{mention}'")

    prompt = (
        f"ä½ ç°åœ¨æ˜¯å†›äº‹é¢†åŸŸä¸“å®¶ï¼Œéœ€è¦å‚ç…§ä»¥ä¸‹ä¾‹å­ç»™å‡ºæåŠå¯¹åº”çš„åˆ«åã€å®šä¹‰å’Œè¯¦ç»†æè¿°ï¼ˆä¸­è‹±æ–‡å„ä¸€ç‰ˆï¼‰ã€‚\n"
        f"ä¾‹å­ï¼š\n"
        f"æåŠï¼šSteyr HS .50\n"
        f"ä¸­æ–‡åˆ«åï¼šæ–¯æ³°å°”HS .50ç‹™å‡»æ­¥æª\n"
        f"è‹±æ–‡åˆ«åï¼šSteyr HS .50 sniper rifle\n"
        f"ä¸­æ–‡å®šä¹‰ï¼šæ–¯æ³°å°”HS .50ï¼ˆSteyr HS.50ï¼‰æ˜¯ç”±å¥¥åœ°åˆ©æ–¯æ³°å°”-æ›¼åˆ©å¤å…¬å¸ç ”åˆ¶çš„ä¸€æ¬¾æ‰‹åŠ¨æªæœºå¼åå™¨æç‹™å‡»æ­¥æªã€‚\n"
        f"è‹±æ–‡å®šä¹‰ï¼šThe Steyr HS .50 (Steyr HS.50) is a manually operated anti-materiel sniper rifle developed by Steyr Mannlicher of Austria.\n"
        f"ä¸­æ–‡è¯¦ç»†æè¿°ï¼šæ–¯æ³°å°”HS .50æ˜¯ä¸€æ¬¾å¤§å£å¾„åå™¨æç‹™å‡»æ­¥æªï¼Œé‡‡ç”¨æ‰‹åŠ¨æªæœºæ“ä½œæ–¹å¼ï¼Œå‘å°„12.7Ã—99æ¯«ç±³ï¼ˆ.50 BMGï¼‰å¼¹è¯ã€‚è¯¥æªå…·æœ‰å‡ºè‰²çš„è¿œè·ç¦»ç²¾ç¡®å°„å‡»èƒ½åŠ›ï¼Œä¸»è¦ç”¨äºåå™¨æä½œæˆ˜å’Œè¿œç¨‹ç‹™å‡»ä»»åŠ¡ã€‚\n"
        f"è‹±æ–‡è¯¦ç»†æè¿°ï¼šThe Steyr HS .50 is a large-caliber anti-materiel sniper rifle with manual bolt action, chambered for 12.7Ã—99mm (.50 BMG) ammunition. It features excellent long-range precision shooting capabilities and is primarily used for anti-materiel operations and long-range sniper missions.\n\n"
        f"è¾“å…¥æåŠï¼š{mention}\n\n"
        f"è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç›´æ¥è¿”å›æ‰€éœ€å†…å®¹ï¼Œä¸è¦è§£é‡Šæˆ–é™„åŠ å†…å®¹ï¼š\n"
        f"æ ‡ç­¾ï¼š{mention}\n"
        f"ä¸­æ–‡åˆ«åï¼š\n"
        f"è‹±æ–‡åˆ«åï¼š\n"
        f"ä¸­æ–‡å®šä¹‰ï¼š\n"
        f"è‹±æ–‡å®šä¹‰ï¼š\n"
        f"ä¸­æ–‡è¯¦ç»†æè¿°ï¼š\n"
        f"è‹±æ–‡è¯¦ç»†æè¿°ï¼š"
    )

    if verbose:
        print(f"\nğŸ“¤ å‘é€Prompt:")
        print(f"{'-' * 60}")
        print(prompt)
        print(f"{'-' * 60}")

    try:
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
        )
        response_content = response.choices[0].message.content.strip()

        if verbose:
            print(f"\nğŸ“¥ LLMå“åº”:")
            print(f"{'-' * 60}")
            print(response_content)
            print(f"{'-' * 60}")

        if not response_content:
            raise ValueError(f"No response content for mention '{mention}'")

        return response_content
    except Exception as e:
        if verbose:
            print(f"\nâŒ LLMè°ƒç”¨å¤±è´¥: {e}")
        raise


def normalize_url(url):
    """å½’ä¸€åŒ–URLï¼Œå¤„ç†URLç¼–ç é—®é¢˜"""
    if not url:
        return ""

    url = str(url).strip()

    if "wikipedia.org/wiki/" in url:
        try:
            if "/wiki/" in url:
                parts = url.split("/wiki/", 1)
                if len(parts) == 2:
                    title = parts[1]
                    try:
                        decoded_title = unquote(title)
                    except:
                        decoded_title = title
                    return decoded_title
        except Exception:
            pass

    return url


def clean_link(link):
    """æ¸…ç†é“¾æ¥ï¼Œç§»é™¤ç©ºç™½å­—ç¬¦å’Œå¸¸è§å‰ç¼€"""
    if not link:
        return ""
    link = str(link).strip()
    link = re.sub(r'^\d+[\.\)]\s*', '', link)
    link = re.sub(r'^é€‰é¡¹\d+[ï¼š:]\s*', '', link)
    link = re.sub(r'^link[ï¼š:]\s*', '', link, flags=re.IGNORECASE)
    return link.strip()


def semantic_entity_match(query, description, verbose=True):
    """
    ä½¿ç”¨LLMè¿›è¡Œç²¾ç¡®çš„è¯­ä¹‰å®ä½“åŒ¹é…
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        description: æ¡ç›®æè¿°æ–‡æœ¬
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        bool: æ˜¯å¦åŒ¹é…
    """
    prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹æè¿°æ˜¯å¦å±äºæŸ¥è¯¢å®ä½“çš„ç±»åˆ«ã€‚åªéœ€å›ç­”"æ˜¯"æˆ–"å¦"ã€‚

æŸ¥è¯¢å®ä½“: "{query}"
æè¿°æ–‡æœ¬: "{description[:500]}"

åˆ¤æ–­æ ‡å‡†:
- å¦‚æœæè¿°æ˜ç¡®æåˆ°å±äºæŸ¥è¯¢å®ä½“ç±»åˆ«ï¼Œå›ç­”"æ˜¯"
- å¦‚æœæè¿°æ˜¯å…³äºæŸ¥è¯¢å®ä½“ç±»åˆ«çš„å…·ä½“å®ä¾‹ï¼Œå›ç­”"æ˜¯"  
- å¦‚æœæè¿°ä¸æŸ¥è¯¢å®ä½“ç±»åˆ«ç›¸å…³ä½†ä¸å±äºï¼Œå›ç­”"å¦"
- å¦‚æœæè¿°ä¸ç›¸å…³ï¼Œå›ç­”"å¦"

ç­”æ¡ˆ: """

    try:
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=10
        )
        answer = response.choices[0].message.content.strip().lower()

        if verbose:
            print(f"   ğŸ¤– LLMåˆ¤æ–­: '{answer}'")

        return "æ˜¯" in answer or "yes" in answer or "true" in answer
    except Exception as e:
        if verbose:
            print(f"   âŒ LLMåŒ¹é…å¤±è´¥: {e}")
        return False


def is_entity_match(query_text, entry_description, verbose=True):
    """
    åˆ¤æ–­æ¡ç›®æ˜¯å¦å±äºæŸ¥è¯¢å®ä½“ç±»åˆ«ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰
    
    ä½¿ç”¨å¤šçº§åŒ¹é…ç­–ç•¥ï¼š
    1. å…³é”®è¯åŒ¹é…ï¼šæå–æŸ¥è¯¢ä¸­çš„æ ¸å¿ƒå®ä½“è¯ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æè¿°ä¸­å‡ºç°
    2. å‘é‡ç›¸ä¼¼åº¦åŒ¹é…ï¼šå¿«é€Ÿè¿‘ä¼¼åŒ¹é…
    3. LLMè¯­ä¹‰åŒ¹é…ï¼šç²¾ç¡®ä½†è¾ƒæ…¢çš„åŒ¹é…
    
    Args:
        query_text: æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¦‚"é˜¿åˆ©Â·ä¼¯å…‹Flight â…¢"ï¼‰
        entry_description: æ¡ç›®æè¿°æ–‡æœ¬
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        bool: æ˜¯å¦åŒ¹é…
    """
    # é¢„å¤„ç†æ–‡æœ¬
    query_text = preprocess_query(query_text)
    entry_description = preprocess_query(entry_description)

    if verbose:
        print(f"\nğŸ” è¯­ä¹‰åŒ¹é…æ£€æŸ¥:")
        print(f"   æŸ¥è¯¢: '{query_text}'")
        print(f"   æ¡ç›®æè¿°: '{entry_description[:200]}...'")

    query_lower = query_text.lower()
    desc_lower = entry_description.lower()

    # 1. ç›´æ¥å…³é”®è¯åŒ¹é…ï¼šæå–æŸ¥è¯¢ä¸­çš„æ ¸å¿ƒå®ä½“è¯
    # ç§»é™¤å¸¸è§ä¿®é¥°è¯ï¼Œæå–æ ¸å¿ƒå®ä½“åç§°
    import re
    
    # æå–å¯èƒ½çš„å®ä½“å…³é”®è¯ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰
    # åŒ¹é…ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å•è¯ã€æ•°å­—ã€è¿å­—ç¬¦ç­‰
    entity_patterns = [
        r'[\u4e00-\u9fa5]+',  # ä¸­æ–‡å­—ç¬¦
        r'[A-Z][a-zA-Z\s-]+',  # è‹±æ–‡ä¸“æœ‰åè¯ï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰
        r'[A-Z]+[0-9]+',  # å‹å·ï¼ˆå¦‚P226, OH-58Dï¼‰
    ]
    
    extracted_terms = []
    for pattern in entity_patterns:
        matches = re.findall(pattern, query_text)
        for match in matches:
            match_clean = match.strip()
            # è¿‡æ»¤å¤ªçŸ­çš„è¯ï¼ˆå°‘äº2ä¸ªå­—ç¬¦ï¼‰å’Œå¸¸è§ä¿®é¥°è¯
            if len(match_clean) >= 2 and match_clean.lower() not in ['çº§', 'å‹', 'å·', 'çš„', 'the', 'a', 'an']:
                extracted_terms.append(match_clean)
    
    # æ£€æŸ¥æå–çš„å…³é”®è¯æ˜¯å¦åœ¨æè¿°ä¸­å‡ºç°
    for term in extracted_terms:
        term_lower = term.lower()
        # å¦‚æœå…³é”®è¯åœ¨æŸ¥è¯¢å’Œæè¿°ä¸­éƒ½å‡ºç°ï¼Œä¸”é•¿åº¦>=3ï¼ˆé¿å…å¤ªçŸ­çš„è¯è¯¯åŒ¹é…ï¼‰
        if len(term) >= 3 and term_lower in query_lower and term_lower in desc_lower:
            if verbose:
                print(f"   âœ… å…³é”®è¯åŒ¹é…: '{term}' (åœ¨æŸ¥è¯¢å’Œæè¿°ä¸­éƒ½å‡ºç°)")
            return True
    
    # 2. åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„åŒ¹é…ï¼ˆå¿«é€Ÿè¿‘ä¼¼ï¼Œä¼˜å…ˆä½¿ç”¨ï¼‰
    if model is not None and tokenizer is not None:
        try:
            query_vec = generate_vector(query_text, use_cache=True, verbose=False)
            desc_vec = generate_vector(entry_description[:500], use_cache=True, verbose=False)

            if query_vec and desc_vec:
                similarity = np.dot(query_vec, desc_vec)
                if verbose:
                    print(f"   ğŸ“Š å‘é‡ç›¸ä¼¼åº¦: {similarity:.4f}")

                if similarity > 0.75:  # æé«˜é˜ˆå€¼åˆ°0.75ï¼Œæ›´ä¸¥æ ¼
                    if verbose:
                        print(f"   âœ… å‘é‡ç›¸ä¼¼åº¦åŒ¹é… (>{0.75})")
                    return True
        except Exception as e:
            if verbose:
                print(f"   âš ï¸  å‘é‡åŒ¹é…å¤±è´¥: {e}")

    # 3. ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰åŒ¹é…ï¼ˆæ›´ç²¾ç¡®ä½†è¾ƒæ…¢ï¼Œä½œä¸ºæœ€åæ‰‹æ®µï¼‰
    if len(entry_description) > 50:  # åªæœ‰æè¿°è¶³å¤Ÿé•¿æ—¶æ‰ä½¿ç”¨LLM
        try:
            return semantic_entity_match(query_text, entry_description, verbose)
        except Exception as e:
            if verbose:
                print(f"   âš ï¸  LLMè¯­ä¹‰åŒ¹é…å¤±è´¥: {e}")

    if verbose:
        print(f"   âŒ æœªåŒ¹é…")
    return False


def check_query_hit(query_text, entry_descriptions, verbose=True):
    """
    æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦å‘½ä¸­æ¡ç›®ï¼ˆæ”¯æŒè¯­ä¹‰åŒ¹é…ï¼‰
    
    Args:
        query_text: æŸ¥è¯¢æ–‡æœ¬
        entry_descriptions: æ¡ç›®æè¿°åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸åˆ—è¡¨ï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        bool: æ˜¯å¦å‘½ä¸­
    """
    if verbose:
        print(f"\n{'=' * 60}")
        print(f"ğŸ¯ è¯­ä¹‰å‘½ä¸­æ£€æŸ¥: '{query_text}'")
        print(f"{'=' * 60}")

    # ç»Ÿä¸€å¤„ç†æ¡ç›®æè¿°
    descriptions = []
    if isinstance(entry_descriptions, str):
        descriptions = [entry_descriptions]
    elif isinstance(entry_descriptions, list):
        if entry_descriptions and isinstance(entry_descriptions[0], dict):
            # ä»å­—å…¸ä¸­æå–æè¿°æ–‡æœ¬
            for entry in entry_descriptions:
                desc = entry.get('descriptions_zh', '') or entry.get('descriptions_en', '') or entry.get('label', '')
                if desc:
                    descriptions.append(desc)
        else:
            descriptions = entry_descriptions

    if not descriptions:
        if verbose:
            print("   âŒ æ— æœ‰æ•ˆæè¿°å¯æ£€æŸ¥")
        return False

    # æ£€æŸ¥æ¯ä¸ªæè¿°ï¼ˆåªæ£€æŸ¥å‰10ä¸ªï¼Œé¿å…å¤ªæ…¢ï¼‰
    hit_count = 0
    check_count = min(10, len(descriptions))
    for i, desc in enumerate(descriptions[:check_count]):
        if is_entity_match(query_text, desc, verbose=verbose and i < 3):  # åªè¯¦ç»†æ‰“å°å‰3ä¸ª
            hit_count += 1
            if verbose:
                print(f"   âœ… å‘½ä¸­æ¡ç›® {i+1}")

    is_hit = hit_count > 0

    if verbose:
        print(f"\nğŸ“Š å‘½ä¸­ç»Ÿè®¡: {hit_count}/{check_count} (æ£€æŸ¥å‰{check_count}ä¸ª)")
        print(f"ğŸ¯ æœ€ç»ˆç»“æœ: {'âœ… å‘½ä¸­' if is_hit else 'âŒ æœªå‘½ä¸­'}")

    return is_hit


def ensure_links_match(sorted_links, original_links, verbose=True):
    """ç¡®ä¿æ’åºåçš„é“¾æ¥ä¸åŸå§‹é“¾æ¥ä¸€è‡´ï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…"""
    if verbose:
        print(f"\n{'=' * 60}")
        print(f"ğŸ”— é“¾æ¥åŒ¹é…å’ŒéªŒè¯")
        print(f"{'=' * 60}")
        print(f"åŸå§‹é“¾æ¥æ•°é‡: {len(original_links)}")
        print(f"LLMè¿”å›é“¾æ¥æ•°é‡: {len(sorted_links)}")

    cleaned_sorted = [clean_link(link) for link in sorted_links]
    original_links_set = set(original_links)

    cleaned_to_original = {}
    for orig_link in original_links:
        cleaned = clean_link(orig_link)
        cleaned_to_original[cleaned] = orig_link

    matched_links = []
    used_original_links = set()
    match_info = []

    for i, cleaned_link in enumerate(cleaned_sorted):
        matched = False
        match_type = None

        # ç²¾ç¡®åŒ¹é…
        if cleaned_link in cleaned_to_original:
            orig_link = cleaned_to_original[cleaned_link]
            if orig_link not in used_original_links:
                matched_links.append(orig_link)
                used_original_links.add(orig_link)
                matched = True
                match_type = "ç²¾ç¡®åŒ¹é…"

        # URLå½’ä¸€åŒ–åŒ¹é…
        if not matched:
            normalized_link = normalize_url(cleaned_link)
            for orig_link in original_links:
                if orig_link not in used_original_links:
                    orig_cleaned = clean_link(orig_link)
                    orig_normalized = normalize_url(orig_cleaned)
                    if normalized_link == orig_normalized:
                        matched_links.append(orig_link)
                        used_original_links.add(orig_link)
                        matched = True
                        match_type = "URLå½’ä¸€åŒ–åŒ¹é…"
                        break

        # æ¨¡ç³ŠåŒ¹é…
        if not matched:
            for orig_link in original_links:
                if orig_link not in used_original_links:
                    orig_cleaned = clean_link(orig_link)
                    if cleaned_link in orig_cleaned or orig_cleaned in cleaned_link:
                        matched_links.append(orig_link)
                        used_original_links.add(orig_link)
                        matched = True
                        match_type = "æ¨¡ç³ŠåŒ¹é…"
                        break

        # å½’ä¸€åŒ–åçš„æ¨¡ç³ŠåŒ¹é…
        if not matched:
            normalized_link = normalize_url(cleaned_link)
            for orig_link in original_links:
                if orig_link not in used_original_links:
                    orig_normalized = normalize_url(clean_link(orig_link))
                    if normalized_link in orig_normalized or orig_normalized in normalized_link:
                        matched_links.append(orig_link)
                        used_original_links.add(orig_link)
                        matched = True
                        match_type = "å½’ä¸€åŒ–æ¨¡ç³ŠåŒ¹é…"
                        break

        if verbose and i < 5:  # åªæ‰“å°å‰5ä¸ªåŒ¹é…ä¿¡æ¯
            if matched:
                print(f"   {i + 1}. âœ… {match_type}: '{cleaned_link[:50]}...' -> å·²åŒ¹é…")
            else:
                print(f"   {i + 1}. âŒ æœªåŒ¹é…: '{cleaned_link[:50]}...'")

    # æ·»åŠ æœªåŒ¹é…çš„åŸå§‹é“¾æ¥
    unmatched_count = 0
    for orig_link in original_links:
        if orig_link not in used_original_links:
            matched_links.append(orig_link)
            unmatched_count += 1

    if verbose:
        print(f"\nâœ… åŒ¹é…å®Œæˆ: {len(matched_links)} ä¸ªé“¾æ¥ï¼ˆå…¶ä¸­ {unmatched_count} ä¸ªæœªåŒ¹é…çš„åŸå§‹é“¾æ¥ï¼‰")

    return matched_links


def generate_prompt_and_sort_with_description(mention, results, verbose=True):
    """
    ä½¿ç”¨LLMé‡æ’åºï¼Œé‡ç‚¹ä½¿ç”¨å®Œæ•´çš„æè¿°ä¿¡æ¯è¿›è¡ŒåŒ¹é…

    Args:
        mention: æŸ¥è¯¢æåŠ
        results: å‘é‡æ£€ç´¢ç»“æœåˆ—è¡¨
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    # é¢„å¤„ç†æŸ¥è¯¢
    mention = preprocess_query(mention)

    if verbose:
        print(f"\n{'=' * 60}")
        print(f"ğŸ¤– LLMé‡æ’åº: '{mention}'")
        print(f"{'=' * 60}")
        print(f"ğŸ“Š è¾“å…¥ç»“æœæ•°é‡: {len(results)}")

    input_label = mention
    response_content = ""

    try:
        response_content = get_alias_and_definition(mention, verbose=verbose)

        # å®‰å…¨æå–å­—æ®µå†…å®¹
        def safe_extract(content, field_name, default=""):
            # å°è¯•ä¸­æ–‡å†’å·
            if f"{field_name}ï¼š" in content:
                parts = content.split(f"{field_name}ï¼š", 1)
                if len(parts) > 1:
                    # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå­—æ®µæ ‡è®°ä½œä¸ºç»“æŸä½ç½®
                    next_markers = ["è‹±æ–‡åˆ«å", "ä¸­æ–‡åˆ«å", "ä¸­æ–‡å®šä¹‰", "è‹±æ–‡å®šä¹‰", "ä¸­æ–‡è¯¦ç»†æè¿°", "è‹±æ–‡è¯¦ç»†æè¿°", "æ ‡ç­¾", "\n\n"]
                    end_pos = len(parts[1])
                    for marker in next_markers:
                        marker_idx = parts[1].find(marker)
                        if marker_idx != -1 and marker_idx < end_pos:
                            end_pos = marker_idx
                    value = parts[1][:end_pos].strip()
                    return value if value else default
            # å°è¯•è‹±æ–‡å†’å·
            elif f"{field_name}:" in content:
                parts = content.split(f"{field_name}:", 1)
                if len(parts) > 1:
                    next_markers = ["è‹±æ–‡åˆ«å", "ä¸­æ–‡åˆ«å", "ä¸­æ–‡å®šä¹‰", "è‹±æ–‡å®šä¹‰", "ä¸­æ–‡è¯¦ç»†æè¿°", "è‹±æ–‡è¯¦ç»†æè¿°", "æ ‡ç­¾", "\n\n"]
                    end_pos = len(parts[1])
                    for marker in next_markers:
                        marker_idx = parts[1].find(marker)
                        if marker_idx != -1 and marker_idx < end_pos:
                            end_pos = marker_idx
                    value = parts[1][:end_pos].strip()
                    return value if value else default
            return default

        input_aliases_zh = safe_extract(response_content, "ä¸­æ–‡åˆ«å", "")
        input_aliases_en = safe_extract(response_content, "è‹±æ–‡åˆ«å", "")
        input_definition_zh = safe_extract(response_content, "ä¸­æ–‡å®šä¹‰", "")
        input_definition_en = safe_extract(response_content, "è‹±æ–‡å®šä¹‰", "")
        input_description_zh = safe_extract(response_content, "ä¸­æ–‡è¯¦ç»†æè¿°", "")
        input_description_en = safe_extract(response_content, "è‹±æ–‡è¯¦ç»†æè¿°", "")

        if verbose:
            print(f"\nğŸ“‹ è§£æç»“æœ:")
            print(f"   æ ‡ç­¾: {input_label}")
            print(f"   ä¸­æ–‡åˆ«å: {input_aliases_zh if input_aliases_zh else 'æ— '}")
            print(f"   è‹±æ–‡åˆ«å: {input_aliases_en if input_aliases_en else 'æ— '}")
            print(f"   ä¸­æ–‡å®šä¹‰: {input_definition_zh if input_definition_zh else 'æ— '}")
            print(f"   è‹±æ–‡å®šä¹‰: {input_definition_en if input_definition_en else 'æ— '}")
            print(f"   ä¸­æ–‡è¯¦ç»†æè¿°: {input_description_zh[:100] if input_description_zh else 'æ— '}...")
            print(f"   è‹±æ–‡è¯¦ç»†æè¿°: {input_description_en[:100] if input_description_en else 'æ— '}...")

        if not input_aliases_zh and not input_aliases_en and not input_definition_zh and not input_definition_en and not input_description_zh and not input_description_en:
            raise ValueError("æ— æ³•ä»LLMå“åº”ä¸­æå–ä»»ä½•æœ‰æ•ˆå­—æ®µ")

    except (ValueError, IndexError, Exception) as e:
        if verbose:
            print(f"\nâŒ LLMè§£æå¤±è´¥: {e}")
            print(f"   å›é€€åˆ°åŸå§‹é¡ºåº")
        return [result['link'] for result in results]

    # æ„å»ºé€‰é¡¹åˆ—è¡¨ï¼Œç¡®ä¿åŒ…å«å®Œæ•´çš„æè¿°ä¿¡æ¯
    options = []
    original_links = []

    if verbose:
        print(f"\nğŸ“ æ„å»ºé€‰é¡¹åˆ—è¡¨...")

    for idx, result in enumerate(results, start=1):
        # è·å–å®Œæ•´çš„æè¿°ä¿¡æ¯ï¼ˆä¸­è‹±æ–‡ï¼‰
        descriptions_zh = result.get('descriptions_zh', '')
        if not descriptions_zh:
            descriptions_zh = "ï¼ˆæ— æè¿°ä¿¡æ¯ï¼‰"
        descriptions_en = result.get('descriptions_en', '')
        if not descriptions_en:
            descriptions_en = "ï¼ˆæ— æè¿°ä¿¡æ¯ï¼‰"

        # æ„å»ºé€‰é¡¹ï¼Œé‡ç‚¹å±•ç¤ºæè¿°ä¿¡æ¯ï¼ˆä¸­è‹±æ–‡ï¼‰
        option = (
            f"é€‰é¡¹{idx}ï¼š\n"
            f"æ ‡ç­¾(label): {result.get('label', '')}\n"
            f"ä¸­æ–‡åˆ«å(aliases_zh): {', '.join(result.get('aliases_zh', [])) if result.get('aliases_zh') else 'æ— '}\n"
            f"è‹±æ–‡åˆ«å(aliases_en): {', '.join(result.get('aliases_en', [])) if result.get('aliases_en') else 'æ— '}\n"
            f"ä¸­æ–‡å®Œæ•´æè¿°(descriptions_zh): {descriptions_zh}\n"
            f"è‹±æ–‡å®Œæ•´æè¿°(descriptions_en): {descriptions_en}\n"
            f"é“¾æ¥(link): {result.get('link', '')}\n"
        )
        options.append(option)
        original_links.append(result.get('link', ''))

        if verbose and idx <= 3:  # åªæ‰“å°å‰3ä¸ªé€‰é¡¹
            print(f"\n   é€‰é¡¹{idx}:")
            print(f"      æ ‡ç­¾: {result.get('label', '')}")
            print(f"      ä¸­æ–‡æè¿°: {descriptions_zh[:100]}...")
            print(f"      è‹±æ–‡æè¿°: {descriptions_en[:100]}...")
            print(f"      é“¾æ¥: {result.get('link', '')}")

    # åˆ¤æ–­è¾“å…¥æ˜¯ç±»åˆ«è¿˜æ˜¯å®ä¾‹
    is_class_query = any(keyword in input_label.lower() for keyword in ['çº§', 'class', 'å‹', 'ç³»åˆ—', 'series'])
    if not is_class_query:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“å®ä¾‹æ ‡è¯†ï¼ˆå¦‚DDG-88, OH-58Dç­‰ï¼‰
        has_instance_id = bool(re.search(r'[A-Z]+[-_]?\d+', input_label) or 
                              re.search(r'[A-Z]{2,3}-\d+', input_label))
        is_class_query = not has_instance_id
    
    # æ„å»ºpromptï¼Œæ˜ç¡®å¼ºè°ƒè¦ä½¿ç”¨æè¿°ä¿¡æ¯è¿›è¡ŒåŒ¹é…ï¼ˆä¸­è‹±æ–‡ï¼‰ï¼Œå¹¶ä¼˜å…ˆé€‰æ‹©ç±»åˆ«é¡µé¢
    class_instruction = ""
    if is_class_query:
        class_instruction = (
            f"ã€å…³é”®åˆ¤æ–­ã€‘æ ¹æ®è¾“å…¥ä¿¡æ¯åˆ†æï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äº**ç±»åˆ«/çº§åˆ«**çš„æŸ¥è¯¢ï¼ˆå¦‚'é˜¿åˆ©Â·ä¼¯å…‹çº§é©±é€èˆ°'ã€'P226æ‰‹æª'ç­‰ï¼‰ï¼Œ"
            f"è€Œä¸æ˜¯å…·ä½“æŸè‰˜èˆ°è‰‡æˆ–æŸæŠŠæªçš„æŸ¥è¯¢ã€‚\n\n"
            f"ã€æ’åºä¼˜å…ˆçº§ã€‘è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ä¼˜å…ˆçº§æ’åºï¼š\n"
            f"1. **æœ€é«˜ä¼˜å…ˆçº§**ï¼šç±»åˆ«/çº§åˆ«çš„æ€»é¡µé¢ï¼ˆæè¿°æ•´ä¸ªçº§åˆ«/ç³»åˆ—çš„ç‰¹å¾ã€å†å²ã€æŠ€æœ¯å‚æ•°ã€å‘å±•å†ç¨‹ç­‰ï¼‰\n"
            f"2. **æ¬¡ä¼˜å…ˆçº§**ï¼šå±äºè¯¥ç±»åˆ«çš„å…·ä½“å®ä¾‹é¡µé¢ï¼ˆå¦‚å…·ä½“æŸè‰˜èˆ°è‰‡ã€æŸæŠŠæªçš„é¡µé¢ï¼‰\n"
            f"3. **æœ€ä½ä¼˜å…ˆçº§**ï¼šç›¸å…³ä½†ä¸å®Œå…¨åŒ¹é…çš„é¡µé¢\n\n"
            f"ã€è¯†åˆ«ç±»åˆ«é¡µé¢çš„ç‰¹å¾ã€‘ç±»åˆ«é¡µé¢é€šå¸¸åŒ…å«ä»¥ä¸‹ç‰¹å¾ï¼š\n"
            f"- æè¿°ä¸­ä½¿ç”¨'çº§'ã€'class'ã€'ç³»åˆ—'ã€'series'ç­‰è¯æ±‡\n"
            f"- æè¿°æ•´ä¸ªçº§åˆ«/ç³»åˆ—çš„å‘å±•å†å²ã€æŠ€æœ¯ç‰¹ç‚¹ã€ç”Ÿäº§æƒ…å†µ\n"
            f"- æ ‡ç­¾(label)é€šå¸¸æ˜¯ç±»åˆ«åç§°ï¼Œè€Œä¸æ˜¯å…·ä½“èˆ°è‰‡/æ­¦å™¨çš„åç§°\n"
            f"- æè¿°ä¸­ä¼šæåˆ°'è¯¥çº§'ã€'è¯¥ç³»åˆ—'ã€'è¯¥å‹'ç­‰è¯æ±‡\n\n"
        )
    else:
        class_instruction = (
            f"ã€å…³é”®åˆ¤æ–­ã€‘æ ¹æ®è¾“å…¥ä¿¡æ¯åˆ†æï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äº**å…·ä½“å®ä¾‹**çš„æŸ¥è¯¢ï¼ˆå¦‚'USS Preble (DDG-88)'ç­‰ï¼‰ï¼Œ"
            f"åº”ä¼˜å…ˆé€‰æ‹©å¯¹åº”çš„å…·ä½“å®ä¾‹é¡µé¢ã€‚\n\n"
        )
    
    prompt = (
        f"ç°åœ¨ä½ æ˜¯å†›äº‹é¢†åŸŸä¸“å®¶ï¼Œéœ€è¦æ ¹æ®è¾“å…¥ä¿¡æ¯ä¸é€‰é¡¹åˆ—è¡¨çš„å€™é€‰çš„åŒ¹é…åº¦è¿›è¡Œä»é«˜åˆ°ä½æ’åºã€‚\n\n"
        f"ã€é‡è¦æç¤º1ï¼šæè¿°ä¿¡æ¯ä¼˜å…ˆã€‘è¯·é‡ç‚¹å‚è€ƒæ¯ä¸ªé€‰é¡¹çš„å®Œæ•´æè¿°ä¿¡æ¯ï¼ˆåŒ…æ‹¬ä¸­æ–‡æè¿°descriptions_zhå’Œè‹±æ–‡æè¿°descriptions_enï¼‰è¿›è¡ŒåŒ¹é…åº¦åˆ¤æ–­ï¼Œ"
        f"æè¿°ä¿¡æ¯åŒ…å«äº†å®ä½“çš„è¯¦ç»†ç‰¹å¾å’Œå®šä¹‰ï¼Œæ¯”æ ‡ç­¾å’Œåˆ«åæ›´èƒ½å‡†ç¡®åæ˜ å®ä½“çš„æœ¬è´¨ç‰¹å¾ã€‚åœ¨åˆ¤æ–­åŒ¹é…åº¦æ—¶ï¼Œæè¿°ä¿¡æ¯çš„æƒé‡åº”è¯¥é«˜äºæ ‡ç­¾å’Œåˆ«åã€‚\n\n"
        f"{class_instruction}"
        f"ã€é‡è¦æç¤º2ï¼šåŒ¹é…åº¦åˆ¤æ–­ã€‘åœ¨åˆ¤æ–­åŒ¹é…åº¦æ—¶ï¼Œè¯·ç»¼åˆè€ƒè™‘ï¼š\n"
        f"- æ ‡ç­¾å’Œåˆ«åæ˜¯å¦ä¸è¾“å…¥ä¿¡æ¯åŒ¹é…\n"
        f"- æè¿°ä¿¡æ¯æ˜¯å¦ä¸è¾“å…¥ä¿¡æ¯çš„å®šä¹‰å’Œè¯¦ç»†æè¿°åŒ¹é…\n"
        f"- å¦‚æœæ˜¯ç±»åˆ«æŸ¥è¯¢ï¼Œæè¿°ä¸­æ˜¯å¦æ˜ç¡®æåˆ°å±äºè¯¥ç±»åˆ«\n\n"
        f"è¾“å…¥ä¿¡æ¯ï¼š\n"
        f"  æ ‡ç­¾åï¼š{input_label}\n"
        f"  ä¸­æ–‡åˆ«åï¼š{input_aliases_zh if input_aliases_zh else 'æ— '}\n"
        f"  è‹±æ–‡åˆ«åï¼š{input_aliases_en if input_aliases_en else 'æ— '}\n"
        f"  ä¸­æ–‡å®šä¹‰ï¼š{input_definition_zh if input_definition_zh else 'æ— '}\n"
        f"  è‹±æ–‡å®šä¹‰ï¼š{input_definition_en if input_definition_en else 'æ— '}\n"
        f"  ä¸­æ–‡è¯¦ç»†æè¿°ï¼š{input_description_zh if input_description_zh else 'æ— '}\n"
        f"  è‹±æ–‡è¯¦ç»†æè¿°ï¼š{input_description_en if input_description_en else 'æ— '}\n\n"
        f"é€‰é¡¹åˆ—è¡¨ï¼š\n"
        f"{''.join(options)}\n\n"
        f"è¯·æ ¹æ®è¾“å…¥ä¿¡æ¯ä¸é€‰é¡¹çš„åŒ¹é…åº¦ï¼ˆç‰¹åˆ«å…³æ³¨ä¸­è‹±æ–‡æè¿°ä¿¡æ¯çš„åŒ¹é…åº¦ï¼Œä»¥åŠç±»åˆ«vså®ä¾‹çš„åŒºåˆ†ï¼‰ï¼Œä»é«˜åˆ°ä½ä¸¥æ ¼è¿”å›æ‰€æœ‰å€™é€‰çš„linkå€¼ã€‚\n"
        f"ã€é‡è¦è¦æ±‚ã€‘\n"
        f"1. å¿…é¡»è¿”å›æ‰€æœ‰{len(options)}ä¸ªé€‰é¡¹çš„linkå€¼ï¼Œä¸èƒ½æœ‰ç¼ºå¤±\n"
        f"2. æ¯ä¸ªlinkå€¼åªèƒ½å‡ºç°ä¸€æ¬¡ï¼Œä¸èƒ½æœ‰é‡å¤\n"
        f"3. åªè¿”å›linkå€¼ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦è§£é‡Šæˆ–é™„åŠ å†…å®¹\n"
        f"4. ç¡®ä¿è¿”å›çš„linkå€¼å®Œå…¨åŒ¹é…é€‰é¡¹åˆ—è¡¨ä¸­çš„linkå€¼\n"
        f"5. å¦‚æœè¾“å…¥æ˜¯ç±»åˆ«/çº§åˆ«ï¼Œä¼˜å…ˆå°†ç±»åˆ«é¡µé¢æ’åœ¨å‰é¢ï¼›å¦‚æœè¾“å…¥æ˜¯å…·ä½“å®ä¾‹ï¼Œä¼˜å…ˆå°†å¯¹åº”å®ä¾‹é¡µé¢æ’åœ¨å‰é¢"
    )

    if verbose:
        print(f"\nğŸ“¤ å‘é€é‡æ’åºPromptï¼ˆé•¿åº¦: {len(prompt)} å­—ç¬¦ï¼‰")
        print(f"{'-' * 60}")
        print(prompt[:500] + "..." if len(prompt) > 500 else prompt)
        print(f"{'-' * 60}")

    try:
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[{"role": "user", "content": prompt}]
        )
        response_text = response.choices[0].message.content.strip()

        if verbose:
            print(f"\nğŸ“¥ LLMé‡æ’åºå“åº”:")
            print(f"{'-' * 60}")
            print(response_text)
            print(f"{'-' * 60}")

        sorted_links_raw = [line.strip() for line in response_text.split("\n") if line.strip()]
        
        # å»é‡ï¼šä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„é“¾æ¥
        seen = set()
        sorted_links_dedup = []
        for link in sorted_links_raw:
            link_normalized = normalize_url(clean_link(link))
            if link_normalized not in seen:
                seen.add(link_normalized)
                sorted_links_dedup.append(link)

        if verbose:
            if len(sorted_links_raw) != len(sorted_links_dedup):
                print(f"\nâš ï¸  LLMè¿”å›äº† {len(sorted_links_raw)} ä¸ªé“¾æ¥ï¼Œå»é‡åä¸º {len(sorted_links_dedup)} ä¸ª")
            print(f"\nğŸ“‹ è§£æåçš„é“¾æ¥åˆ—è¡¨ï¼ˆå‰5ä¸ªï¼‰:")
            for i, link in enumerate(sorted_links_dedup[:5], 1):
                print(f"   {i}. {link[:80]}...")

        sorted_links = ensure_links_match(sorted_links_dedup, original_links, verbose=verbose)

        if verbose:
            print(f"\nâœ… æœ€ç»ˆæ’åºç»“æœï¼ˆå‰5ä¸ªï¼‰:")
            for i, link in enumerate(sorted_links[:5], 1):
                print(f"   {i}. {link[:80]}...")

        return sorted_links
    except Exception as e:
        if verbose:
            print(f"\nâŒ LLMæ’åºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        return original_links


def find_rank(correct_link, sorted_links, verbose=True):
    """æŸ¥æ‰¾æ­£ç¡®é“¾æ¥åœ¨æ’åºåˆ—è¡¨ä¸­çš„æ’å"""
    if verbose:
        print(f"\n{'=' * 60}")
        print(f"ğŸ¯ æŸ¥æ‰¾æ­£ç¡®é“¾æ¥æ’å")
        print(f"{'=' * 60}")
        print(f"æ­£ç¡®é“¾æ¥: {correct_link}")

    rank = None
    correct_link_cleaned = clean_link(str(correct_link))
    correct_link_normalized = normalize_url(correct_link_cleaned)

    if verbose:
        print(f"æ¸…ç†å: {correct_link_cleaned}")
        print(f"å½’ä¸€åŒ–å: {correct_link_normalized}")
        print(f"\nå¼€å§‹åŒ¹é…æ£€æŸ¥...")

    for i, link in enumerate(sorted_links):
        link_cleaned = clean_link(str(link))
        link_normalized = normalize_url(link_cleaned)

        # å¤šç§åŒ¹é…æ–¹å¼
        matched = False
        match_type = None

        # 1. å½’ä¸€åŒ–åçš„URLåŒ¹é…
        if correct_link_normalized == link_normalized:
            rank = i + 1
            matched = True
            match_type = "URLå½’ä¸€åŒ–åŒ¹é…"

        # 2. æ¸…ç†åçš„ç²¾ç¡®åŒ¹é…
        if not matched and correct_link_cleaned == link_cleaned:
            rank = i + 1
            matched = True
            match_type = "ç²¾ç¡®åŒ¹é…"

        # 3. åŒå‘å­å­—ç¬¦ä¸²åŒ¹é…
        if not matched and (correct_link_cleaned in link_cleaned or link_cleaned in correct_link_cleaned):
            rank = i + 1
            matched = True
            match_type = "å­å­—ç¬¦ä¸²åŒ¹é…"

        # 4. å½’ä¸€åŒ–åçš„åŒå‘åŒ¹é…
        if not matched and (correct_link_normalized in link_normalized or link_normalized in correct_link_normalized):
            rank = i + 1
            matched = True
            match_type = "å½’ä¸€åŒ–å­å­—ç¬¦ä¸²åŒ¹é…"

        if matched:
            if verbose:
                print(f"\nâœ… æ‰¾åˆ°åŒ¹é…ï¼")
                print(f"   æ’å: {rank}")
                print(f"   åŒ¹é…ç±»å‹: {match_type}")
                print(f"   åŒ¹é…é“¾æ¥: {link}")
            break

        if verbose and i < 10:  # æ‰“å°å‰10ä¸ªå°è¯•
            print(f"   ä½ç½® {i + 1}: '{link[:80]}...' - ä¸åŒ¹é…")
            print(f"      å½’ä¸€åŒ–å: '{link_normalized[:60]}...'")

    if rank is None:
        if verbose:
            print(f"\nâŒ æœªæ‰¾åˆ°åŒ¹é…")
            print(f"   å·²æ£€æŸ¥ {len(sorted_links)} ä¸ªé“¾æ¥")
            print(f"   æ­£ç¡®ç­”æ¡ˆå½’ä¸€åŒ–å: '{correct_link_normalized}'")
            print(f"\n   æ’åºåˆ—è¡¨ä¸­çš„æ‰€æœ‰é“¾æ¥ï¼ˆå‰20ä¸ªï¼‰:")
            for i, link in enumerate(sorted_links[:20], 1):
                link_norm = normalize_url(clean_link(link))
                print(f"      {i}. {link}")
                print(f"         å½’ä¸€åŒ–: {link_norm}")

    return rank


def process_single_query(query, correct_link, use_llm=True, verbose=True):
    """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
    print(f"\n{'#' * 80}")
    print(f"# å¤„ç†æŸ¥è¯¢: '{query}'")
    print(f"# æ­£ç¡®ç­”æ¡ˆ: '{correct_link}'")
    print(f"# ä½¿ç”¨LLMé‡æ’åº: {use_llm}")
    print(f"{'#' * 80}")

    # 1. å‘é‡æ£€ç´¢ï¼ˆä½¿ç”¨æ‰€æœ‰å‘é‡å­—æ®µï¼‰
    results = vector_search(query, top_k=30, verbose=verbose)  # å¢åŠ åˆ°30ï¼ŒåŒæ—¶æ£€ç´¢æ‰€æœ‰7ä¸ªå‘é‡å­—æ®µï¼Œç»™LLMæ›´å¤šå€™é€‰

    if not results:
        print(f"\nâŒ å‘é‡æ£€ç´¢æœªæ‰¾åˆ°ç»“æœ")
        return None, 0, 0, 0, 0

    # 2. LLMé‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if use_llm:
        sorted_links = generate_prompt_and_sort_with_description(query, results, verbose=verbose)
    else:
        sorted_links = [r.get("link", "") for r in results]
        if verbose:
            print(f"\nğŸ“‹ ç›´æ¥ä½¿ç”¨å‘é‡æ£€ç´¢é¡ºåºï¼ˆä¸ä½¿ç”¨LLMé‡æ’åºï¼‰")
            print(f"   å‰5ä¸ªé“¾æ¥:")
            for i, link in enumerate(sorted_links[:5], 1):
                print(f"      {i}. {link[:80]}...")

    # 3. æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨ESä¸­ï¼Œå¹¶è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ï¼ˆè°ƒè¯•ç”¨ï¼‰
    if verbose:
        print(f"\n{'=' * 60}")
        print(f"ğŸ” æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨ESä¸­å¹¶è®¡ç®—å‘é‡ç›¸ä¼¼åº¦")
        print(f"{'=' * 60}")
        correct_link_normalized = normalize_url(clean_link(str(correct_link)))
        found_in_es = False
        found_in_results = False
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆç”¨äºè®¡ç®—ç›¸ä¼¼åº¦ï¼‰
        query_vec = None
        if model is not None and tokenizer is not None:
            try:
                query_vec = generate_vector(query, use_cache=True, verbose=False)
            except Exception as e:
                print(f"âš ï¸  ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {e}")
        
        # å°è¯•åœ¨ESä¸­ç›´æ¥æœç´¢è¿™ä¸ªé“¾æ¥ï¼ˆä½¿ç”¨å¤šç§æ–¹å¼åŒ¹é…ï¼‰
        try:
            # æ–¹æ³•1: ä½¿ç”¨termæŸ¥è¯¢ç²¾ç¡®åŒ¹é…ï¼ˆå¦‚æœlinkå­—æ®µæœ‰keywordå­å­—æ®µï¼‰
            # æ˜¾å¼æŒ‡å®šè¦è¿”å›çš„å‘é‡å­—æ®µ
            vector_fields_list = [
                "descriptions_zh_vector", "descriptions_en_vector",
                "high_freq_words_zh_vector", "high_freq_words_en_vector",
                "label_vector", "label_zh_vector", "label_en_vector"
            ]
            search_query = {
                "query": {
                    "term": {
                        "link.keyword": correct_link
                    }
                },
                "_source": ["label", "link"] + vector_fields_list,  # æ˜¾å¼æŒ‡å®šè¦è¿”å›çš„å­—æ®µ
                "size": 10  # å¤šè¿”å›ä¸€äº›ï¼Œç”¨äºåŒ¹é…
            }
            resp = es.search(index="data2", body=search_query)
            hits = resp.get("hits", {}).get("hits", [])
            
            # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•match_phraseæŸ¥è¯¢
            if not hits:
                search_query = {
                    "query": {
                        "match_phrase": {
                            "link": correct_link
                        }
                    },
                    "_source": ["label", "link"] + vector_fields_list,  # æ˜¾å¼æŒ‡å®šè¦è¿”å›çš„å­—æ®µ
                    "size": 10
                }
                resp = es.search(index="data2", body=search_query)
                hits = resp.get("hits", {}).get("hits", [])
            
            # æ–¹æ³•3: å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨å½’ä¸€åŒ–åçš„URLè¿›è¡ŒåŒ¹é…
            if not hits:
                # è·å–æ‰€æœ‰æ–‡æ¡£ï¼Œç„¶åæ‰‹åŠ¨åŒ¹é…ï¼ˆå› ä¸ºESå¯èƒ½å­˜å‚¨çš„æ˜¯ç¼–ç åçš„URLï¼‰
                search_query = {
                    "query": {"match_all": {}},
                    "_source": ["label", "link"] + vector_fields_list,  # æ˜¾å¼æŒ‡å®šè¦è¿”å›çš„å­—æ®µ
                    "size": 1000  # é™åˆ¶æ•°é‡ï¼Œé¿å…æŸ¥è¯¢å¤ªæ…¢
                }
                resp = es.search(index="data2", body=search_query)
                all_hits = resp.get("hits", {}).get("hits", [])
                
                # æ‰‹åŠ¨åŒ¹é…å½’ä¸€åŒ–åçš„URL
                for hit in all_hits:
                    source = hit.get("_source", {})
                    doc_link = source.get("link", "")
                    if doc_link:
                        doc_link_normalized = normalize_url(clean_link(str(doc_link)))
                        if correct_link_normalized == doc_link_normalized:
                            hits = [hit]
                            break
            if hits:
                found_in_es = True
                hit = hits[0]
                source = hit.get("_source", {})
                print(f"âœ… æ­£ç¡®ç­”æ¡ˆåœ¨ESä¸­æ‰¾åˆ°:")
                print(f"   æ ‡ç­¾: {source.get('label', 'N/A')}")
                print(f"   é“¾æ¥: {source.get('link', 'N/A')}")
                
                # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ï¼ˆè¯­ä¹‰å¯¹é½æ£€æŸ¥ï¼‰
                if query_vec:
                    print(f"\nğŸ“Š å‘é‡ç›¸ä¼¼åº¦åˆ†æï¼ˆè¯­ä¹‰å¯¹é½æ£€æŸ¥ï¼‰:")
                    print(f"   æŸ¥è¯¢æ–‡æœ¬: '{query}'")
                    
                    # æ£€æŸ¥å„ä¸ªå‘é‡å­—æ®µçš„ç›¸ä¼¼åº¦
                    vector_fields_to_check = [
                        ("descriptions_zh_vector", "ä¸­æ–‡æè¿°å‘é‡"),
                        ("descriptions_en_vector", "è‹±æ–‡æè¿°å‘é‡"),
                        ("high_freq_words_zh_vector", "ä¸­æ–‡é«˜é¢‘è¯å‘é‡"),
                        ("high_freq_words_en_vector", "è‹±æ–‡é«˜é¢‘è¯å‘é‡"),
                        ("label_vector", "æ ‡ç­¾å‘é‡"),
                        ("label_zh_vector", "ä¸­æ–‡æ ‡ç­¾å‘é‡"),
                        ("label_en_vector", "è‹±æ–‡æ ‡ç­¾å‘é‡")
                    ]
                    
                    max_similarity = 0
                    best_field = None
                    
                    for field_name, field_desc in vector_fields_to_check:
                        doc_vector = source.get(field_name)
                        if doc_vector and isinstance(doc_vector, list) and len(doc_vector) == len(query_vec):
                            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼Œå› ä¸ºéƒ½å·²å½’ä¸€åŒ–ï¼‰
                            similarity = np.dot(query_vec, doc_vector)
                            if similarity > max_similarity:
                                max_similarity = similarity
                                best_field = field_name
                            print(f"   {field_desc}: {similarity:.4f}")
                        else:
                            print(f"   {field_desc}: æ— å‘é‡æ•°æ®")
                    
                    if best_field:
                        print(f"\n   âœ… æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.4f} (å­—æ®µ: {best_field})")
                        if max_similarity < 0.7:
                            print(f"   âš ï¸  ç›¸ä¼¼åº¦è¾ƒä½ï¼ˆ<0.7ï¼‰ï¼Œå¯èƒ½å¯¼è‡´æ£€ç´¢æ•ˆæœä¸ä½³")
                            print(f"   å»ºè®®ï¼šæ£€æŸ¥æŸ¥è¯¢æ–‡æœ¬å’Œç´¢å¼•æ–‡æœ¬çš„æ ¼å¼æ˜¯å¦ä¸€è‡´")
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ£€ç´¢ç»“æœä¸­
                for i, result in enumerate(results):
                    result_link = result.get("link", "")
                    result_link_normalized = normalize_url(clean_link(str(result_link)))
                    if correct_link_normalized == result_link_normalized:
                        found_in_results = True
                        print(f"\nâœ… æ­£ç¡®ç­”æ¡ˆåœ¨æ£€ç´¢ç»“æœä¸­ï¼ˆä½ç½®: {i+1}, åˆ†æ•°: {result.get('_score', 0):.4f}ï¼‰")
                        break
                
                if not found_in_results:
                    print(f"\nâš ï¸  æ­£ç¡®ç­”æ¡ˆåœ¨ESä¸­ï¼Œä½†ä¸åœ¨top_{len(results)}æ£€ç´¢ç»“æœä¸­")
                    if query_vec and max_similarity < 0.7:
                        print(f"   å¯èƒ½åŸå› ï¼šå‘é‡ç›¸ä¼¼åº¦è¾ƒä½ï¼ˆ{max_similarity:.4f}ï¼‰ï¼Œæ­£ç¡®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦åˆ†æ•°å¯èƒ½ä½äºå…¶ä»–æ–‡æ¡£")
            else:
                print(f"âŒ æ­£ç¡®ç­”æ¡ˆä¸åœ¨ESç´¢å¼•ä¸­")
        except Exception as e:
            print(f"âš ï¸  æ£€æŸ¥ESæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    # 4. è¯­ä¹‰å‘½ä¸­æ£€æŸ¥
    semantic_hit = False
    if verbose:
        print(f"\n{'=' * 60}")
        print(f"ğŸ” è¯­ä¹‰å‘½ä¸­æ£€æŸ¥")
        print(f"{'=' * 60}")
    
    # ä½¿ç”¨æ–°çš„è¯­ä¹‰åŒ¹é…é€»è¾‘
    try:
        semantic_hit = check_query_hit(query, results, verbose=verbose)
    except Exception as e:
        if verbose:
            print(f"âš ï¸  è¯­ä¹‰åŒ¹é…æ£€æŸ¥å¤±è´¥: {e}")
        semantic_hit = False

    # 5. æŸ¥æ‰¾æ’å
    rank = find_rank(correct_link, sorted_links, verbose=verbose)

    # 6. è®¡ç®—æŒ‡æ ‡
    if rank is not None:
        mrr = 1 / rank
        hit_at_1 = 1 if rank <= 1 else 0
        hit_at_5 = 1 if rank <= 5 else 0
        hit_at_10 = 1 if rank <= 10 else 0
    else:
        mrr = 0
        hit_at_1 = 0
        hit_at_5 = 0
        hit_at_10 = 0

    # 7. æ˜¾ç¤ºç»“æœ
    print(f"\n{'=' * 80}")
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"{'=' * 80}")
    print(f"   æŸ¥è¯¢: {query}")
    print(f"   æ­£ç¡®ç­”æ¡ˆ: {correct_link}")
    print(f"   æ’å: {rank if rank else 'æœªæ‰¾åˆ°'}")
    print(f"   è¯­ä¹‰å‘½ä¸­: {'âœ… æ˜¯' if semantic_hit else 'âŒ å¦'}")
    print(f"   MRR: {mrr:.4f}")
    print(f"   Hit@1: {hit_at_1}")
    print(f"   Hit@5: {hit_at_5}")
    print(f"   Hit@10: {hit_at_10}")
    print(f"{'=' * 80}")

    return {
        "query": query,
        "correct_link": correct_link,
        "rank": rank,
        "semantic_hit": 1 if semantic_hit else 0,  # æ–°å¢å­—æ®µ
        "mrr": mrr,
        "hit@1": hit_at_1,
        "hit@5": hit_at_5,
        "hit@10": hit_at_10,
        "sorted_links": sorted_links[:10]
    }, mrr, hit_at_1, hit_at_5, hit_at_10


def read_excel(file_path, max_rows=5):
    """è¯»å–Excelæµ‹è¯•é›†æ–‡ä»¶ï¼ˆé™åˆ¶è¡Œæ•°ç”¨äºè°ƒè¯•ï¼‰"""
    df = pd.read_excel(file_path, header=None)
    queries = df[0].tolist()[:max_rows]
    correct_links = df[1].tolist()[:max_rows]
    print(f"ğŸ“– è¯»å–æµ‹è¯•é›†: {len(queries)} ä¸ªæŸ¥è¯¢ï¼ˆé™åˆ¶ä¸ºå‰{max_rows}æ¡ï¼‰")
    return queries, correct_links


def main():
    """ä¸»å‡½æ•°ï¼šä½¿ç”¨å°‘é‡æµ‹è¯•é›†éªŒè¯æµç¨‹"""
    import sys

    file_path = "data/find.xlsx"
    max_test_rows = 5  # åªæµ‹è¯•å‰5æ¡

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    use_llm = True
    if len(sys.argv) > 1:
        if sys.argv[1] == "--no-llm":
            use_llm = False
        elif sys.argv[1].startswith("--rows="):
            max_test_rows = int(sys.argv[1].split("=")[1])

    if not os.path.exists(file_path):
        print(f"âŒ æœªæ‰¾åˆ°è¯„æµ‹æ–‡ä»¶: {file_path}")
        print("è¯·ç¡®ä¿æµ‹è¯•æ–‡ä»¶å­˜åœ¨")
        return

    # é‡å®šå‘stdoutåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
    class DualOutput:
        def __init__(self, filepath):
            self.terminal = sys.__stdout__  # ä½¿ç”¨åŸå§‹stdout
            self.log_file = open(filepath, "w", encoding="utf-8")
            self._closed = False

        def write(self, message):
            if not self._closed:
                try:
                    self.terminal.write(message)
                    self.log_file.write(message)
                    self.terminal.flush()
                    self.log_file.flush()
                except Exception:
                    pass  # å¿½ç•¥å†™å…¥é”™è¯¯

        def flush(self):
            if not self._closed:
                try:
                    self.terminal.flush()
                    self.log_file.flush()
                except Exception:
                    pass  # å¿½ç•¥åˆ·æ–°é”™è¯¯

        def close(self):
            if not self._closed:
                try:
                    self.log_file.close()
                    self._closed = True
                except Exception:
                    pass  # å¿½ç•¥å…³é—­é”™è¯¯

        def is_closed(self):
            return self._closed

    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = os.path.join(output_dir, f"debug_output_{timestamp}.md")
    
    # åˆ›å»ºåŒè¾“å‡ºå¯¹è±¡
    dual_output = DualOutput(output_filename)
    original_stdout = sys.stdout
    sys.stdout = dual_output

    try:
        queries, correct_links = read_excel(file_path, max_rows=max_test_rows)

        print(f"\n{'=' * 80}")
        print(f"ğŸš€ å¼€å§‹è°ƒè¯•æµ‹è¯•")
        print(f"{'=' * 80}")
        print(f"æµ‹è¯•æŸ¥è¯¢æ•°é‡: {len(queries)}")
        print(f"ä½¿ç”¨LLMé‡æ’åº: {use_llm}")
        print(f"{'=' * 80}\n")

        all_results = []
        total_mrr = 0
        total_hit1 = 0
        total_hit5 = 0
        total_hit10 = 0
        total_semantic_hit = 0

        for i, (query, correct_link) in enumerate(zip(queries, correct_links), 1):
            print(f"\n\n{'=' * 80}")
            print(f"æŸ¥è¯¢ {i}/{len(queries)}")
            print(f"{'=' * 80}")

            result, mrr, hit1, hit5, hit10 = process_single_query(
                query, correct_link, use_llm=use_llm, verbose=True
            )

            if result:
                all_results.append(result)
                total_mrr += mrr
                total_hit1 += hit1
                total_hit5 += hit5
                total_hit10 += hit10
                total_semantic_hit += result.get("semantic_hit", 0)

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if len(all_results) > 0:
            avg_mrr = total_mrr / len(all_results)
            avg_hit1 = total_hit1 / len(all_results)
            avg_hit5 = total_hit5 / len(all_results)
            avg_hit10 = total_hit10 / len(all_results)
            avg_semantic_hit = total_semantic_hit / len(all_results)

            print(f"\n\n{'=' * 80}")
            print(f"ğŸ“Š æ€»ä½“è¯„ä¼°ç»“æœ")
            print(f"{'=' * 80}")
            print(f"æµ‹è¯•æŸ¥è¯¢æ•°é‡: {len(all_results)}")
            print(f"å¹³å‡ MRR: {avg_mrr:.4f}")
            print(f"å¹³å‡ Hit@1: {avg_hit1:.4f}")
            print(f"å¹³å‡ Hit@5: {avg_hit5:.4f}")
            print(f"å¹³å‡ Hit@10: {avg_hit10:.4f}")
            print(f"å¹³å‡è¯­ä¹‰å‘½ä¸­ç‡: {avg_semantic_hit:.4f}")
            print(f"{'=' * 80}")

            # ä¿å­˜ç»“æœ
            report = {
                "timestamp": datetime.now().isoformat(),
                "test_mode": "debug",
                "use_llm": use_llm,
                "total_queries": len(all_results),
                "metrics": {
                    "mrr": avg_mrr,
                    "hit@1": avg_hit1,
                    "hit@5": avg_hit5,
                    "hit@10": avg_hit10,
                    "semantic_hit_rate": avg_semantic_hit
                },
                "detailed_results": all_results
            }

            filename = os.path.join(output_dir, f'debug_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        else:
            print(f"\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æŸ¥è¯¢")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¢å¤åŸå§‹stdout
        sys.stdout = original_stdout
        # å…³é—­æ–‡ä»¶
        dual_output.close()
        print(f"\nâœ… æ§åˆ¶å°è¾“å‡ºå·²ä¿å­˜åˆ°: {output_filename}")

if __name__ == "__main__":
    main()

