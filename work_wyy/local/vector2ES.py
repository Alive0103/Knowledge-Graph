import json
import os
import sys
import torch
import numpy as np
from transformers import BertTokenizer, BertModel
from elasticsearch import helpers
from tqdm import tqdm
import time
import logging
from datetime import datetime

# å¯¼å…¥é…ç½®å’ŒESå®¢æˆ·ç«¯
try:
    from config import (
        ES_INDEX_NAME,
        ENTITY_WORDS_ZH_FILE,
        ENTITY_WORDS_EN_FILE,
        TRAINLOG_DIR,
        VECTOR_DIMS,
        VECTOR_BATCH_SIZE,
        USE_FINETUNED_FOR_VECTORIZATION
    )
    from es_client import es
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
    ES_INDEX_NAME = 'data2'
    ENTITY_WORDS_ZH_FILE = None
    ENTITY_WORDS_EN_FILE = None
    TRAINLOG_DIR = None
    VECTOR_DIMS = 1024
    VECTOR_BATCH_SIZE = 64
    USE_FINETUNED_FOR_VECTORIZATION = True
    from es_client import es

# è®¾ç½®æ—¥å¿—è®°å½• - åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
logger.handlers = []

# æ–‡ä»¶å¤„ç†å™¨
file_handler = logging.FileHandler('vector_import_log.txt', mode='a', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

# æ§åˆ¶å°å¤„ç†å™¨
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)

# æ·»åŠ å¤„ç†å™¨
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# ä½¿ç”¨ç»Ÿä¸€çš„å‘é‡ç”Ÿæˆæ¨¡å—ï¼ˆæ”¯æŒå¾®è°ƒåçš„æ¨¡å‹ï¼‰
VECTOR_DIMS = 1024  # ESå‘é‡å­—æ®µç»´åº¦

# å¯¼å…¥å‘é‡ç”Ÿæˆæ¨¡å—
try:
    import sys
    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if parent_dir not in sys.path:
        sys.path.insert(0, parent_dir)
    from vector_model import load_vector_model, generate_vector as _generate_vector_module, batch_generate_vectors
    
    # åŠ è½½æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ï¼‰
    model, tokenizer, device = load_vector_model(use_finetuned=True)
    print(f"\nâœ… å‘é‡ç”Ÿæˆæ¨¡å‹åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ï¼‰")
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"   é¢„æœŸå‘é‡ç»´åº¦: {VECTOR_DIMS}")
    print("=" * 60)
except Exception as e:
    print(f"âš ï¸  ç»Ÿä¸€å‘é‡æ¨¡å—åŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨åŸºç¡€æ¨¡å‹")
    try:
        model_name = './model/chinese-roberta-wwm-ext-large'
        tokenizer = BertTokenizer.from_pretrained(model_name)
        model = BertModel.from_pretrained(model_name)
        model.eval()
        has_cuda = torch.cuda.is_available()
        if has_cuda:
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
        model = model.to(device)
        print(f"âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
        print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"   é¢„æœŸå‘é‡ç»´åº¦: {VECTOR_DIMS}")
        print("=" * 60)
    except Exception as e2:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e2}")
        exit(1)

# ESç´¢å¼•åç§°ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
INDEX_NAME = ES_INDEX_NAME


def create_vector_index():
    """åˆ›å»ºåŒ…å«å‘é‡å­—æ®µçš„ç´¢å¼•æ˜ å°„ - åŒ…å«æ ‡ç­¾ã€æè¿°ã€å®ä½“è¯å‘é‡"""
    index_mapping = {
        "mappings": {
            "properties": {
                "label": {"type": "text"},
                "link": {"type": "keyword"},
                "aliases_en": {"type": "text"},
                "aliases_zh": {"type": "text"},
                "descriptions_en": {"type": "text"},
                "descriptions_zh": {"type": "text"},
                "content": {"type": "text"},
                # æ ‡ç­¾å‘é‡
                "label_vector": {
                    "type": "dense_vector",
                    "dims": VECTOR_DIMS,
                    "index": True,
                    "similarity": "cosine"
                },
                # ä¸­æ–‡æè¿°å‘é‡
                "descriptions_zh_vector": {
                    "type": "dense_vector",
                    "dims": VECTOR_DIMS,
                    "index": True,
                    "similarity": "cosine"
                },
                # è‹±æ–‡æè¿°å‘é‡
                "descriptions_en_vector": {
                    "type": "dense_vector",
                    "dims": VECTOR_DIMS,
                    "index": True,
                    "similarity": "cosine"
                },
                # ä¸­æ–‡å®ä½“è¯å‘é‡
                "entity_words_zh_vector": {
                    "type": "dense_vector",
                    "dims": VECTOR_DIMS,
                    "index": True,
                    "similarity": "cosine"
                },
                # è‹±æ–‡å®ä½“è¯å‘é‡
                "entity_words_en_vector": {
                    "type": "dense_vector",
                    "dims": VECTOR_DIMS,
                    "index": True,
                    "similarity": "cosine"
                },
                # ä¿ç•™åŸæœ‰å­—æ®µï¼ˆå…¼å®¹æ€§ï¼‰
                "label_zh_vector": {
                    "type": "dense_vector",
                    "dims": VECTOR_DIMS,
                    "index": True,
                    "similarity": "cosine"
                },
                "label_en_vector": {
                    "type": "dense_vector",
                    "dims": VECTOR_DIMS,
                    "index": True,
                    "similarity": "cosine"
                }
            }
        }
    }

    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
    if es.indices.exists(index=INDEX_NAME):
        print(f"åˆ é™¤ç°æœ‰ç´¢å¼•: {INDEX_NAME}")
        try:
            es.indices.delete(index=INDEX_NAME)
            time.sleep(2)
        except Exception as e:
            print(f"åˆ é™¤ç´¢å¼•å¤±è´¥: {e}")
            return False

    # åˆ›å»ºæ–°ç´¢å¼•
    try:
        # å…¼å®¹æ–°æ—§ç‰ˆæœ¬çš„ ES API
        try:
            es.indices.create(index=INDEX_NAME, body=index_mapping)
        except TypeError:
            # æ–°ç‰ˆæœ¬ API ä½¿ç”¨ mappings å‚æ•°
            es.indices.create(index=INDEX_NAME, mappings=index_mapping.get("mappings", {}))
        print(f"æˆåŠŸåˆ›å»ºå‘é‡ç´¢å¼•: {INDEX_NAME}, å‘é‡ç»´åº¦: {VECTOR_DIMS}")
        return True
    except Exception as e:
        print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥è¯¦æƒ…: {e}")
        return False


def generate_vector(text):
    """ç”Ÿæˆæ–‡æœ¬å‘é‡ - ä½¿ç”¨ç»Ÿä¸€çš„å‘é‡ç”Ÿæˆæ¨¡å—ï¼ˆæ”¯æŒå¾®è°ƒåçš„æ¨¡å‹ï¼‰

    æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°åœ¨æ¨¡å—åŠ è½½æ—¶ä¼šè¢«è°ƒç”¨è¿›è¡Œæµ‹è¯•ï¼Œæ‰€ä»¥ä¸èƒ½ä¾èµ–å…¨å±€å˜é‡
    """
    if text and text.strip():
        try:
            # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„å‘é‡ç”Ÿæˆæ¨¡å—
            try:
                vector_list = _generate_vector_module(text, use_finetuned=True, target_dim=VECTOR_DIMS)
                if vector_list and len(vector_list) == VECTOR_DIMS:
                    return [float(x) for x in vector_list]
            except:
                pass
            
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            with torch.no_grad():
                outputs = model(**inputs)
            vector = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()
            
            if len(vector.shape) == 0:
                vector = vector.reshape(1)
            elif len(vector.shape) > 1:
                vector = vector.flatten()
            
            actual_dims = len(vector)
            if actual_dims != VECTOR_DIMS:
                logger.warning(f"å‘é‡ç»´åº¦ä¸åŒ¹é…! æœŸæœ›: {VECTOR_DIMS}, å®é™…: {actual_dims}")
                return None
            
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm
            else:
                return None
            
            vector_list = [float(x) for x in vector.tolist()]
            return vector_list if len(vector_list) == VECTOR_DIMS else None
            
        except Exception as e:
            logger.warning(f"å‘é‡ç”Ÿæˆå¤±è´¥: {e}, æ–‡æœ¬: {text[:50] if text else 'None'}")
            return None
    return None


def generate_vectors_batch(texts, batch_size=32):
    """
    æ‰¹é‡ç”Ÿæˆå‘é‡ - ä½¿ç”¨ç»Ÿä¸€çš„å‘é‡ç”Ÿæˆæ¨¡å—ï¼ˆæ”¯æŒå¾®è°ƒåçš„æ¨¡å‹ï¼‰
    """
    # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„æ‰¹é‡å‘é‡ç”Ÿæˆæ¨¡å—
    try:
        vectors = batch_generate_vectors(
            texts,
            use_finetuned=True,
            target_dim=VECTOR_DIMS,
            batch_size=batch_size
        )
        # è½¬æ¢ä¸ºfloatç±»å‹å¹¶éªŒè¯ç»´åº¦
        result = []
        for vec in vectors:
            if vec and len(vec) == VECTOR_DIMS:
                result.append([float(x) for x in vec])
            else:
                result.append(None)
        return result
    except:
        pass
    
    # å›é€€åˆ°åŸå§‹æ–¹æ³•
    """æ‰¹é‡ç”Ÿæˆå‘é‡ - æ˜¾è‘—æå‡é€Ÿåº¦ï¼ˆGPUæ¨¡å¼ä¸‹å¯æå‡3-10å€ï¼‰
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆGPUå»ºè®®32-64ï¼ŒCPUå»ºè®®8-16ï¼‰
    
    Returns:
        vectors: å‘é‡åˆ—è¡¨ï¼Œä¸è¾“å…¥æ–‡æœ¬ä¸€ä¸€å¯¹åº”ï¼Œå¤±è´¥è¿”å›None
    """
    
    if not texts:
        return []
    
    # è¿‡æ»¤ç©ºæ–‡æœ¬
    valid_texts = [(i, text) for i, text in enumerate(texts) if text and text.strip()]
    if not valid_texts:
        return [None] * len(texts)
    
    vectors = [None] * len(texts)
    
    # æ‰¹é‡å¤„ç†
    for batch_start in range(0, len(valid_texts), batch_size):
        batch_end = min(batch_start + batch_size, len(valid_texts))
        batch_indices = [idx for idx, _ in valid_texts[batch_start:batch_end]]
        batch_texts = [text for _, text in valid_texts[batch_start:batch_end]]
        
        try:
            # æ‰¹é‡tokenize
            inputs = tokenizer(
                batch_texts, 
                return_tensors='pt', 
                padding=True, 
                truncation=True, 
                max_length=512
            )
            # ç§»åˆ°GPU
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs)
            
            # æå–[CLS] tokençš„å‘é‡
            batch_vectors = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            # å¤„ç†æ¯ä¸ªå‘é‡
            for i, (orig_idx, vector) in enumerate(zip(batch_indices, batch_vectors)):
                # ç¡®ä¿æ˜¯ä¸€ç»´æ•°ç»„
                if len(vector.shape) > 1:
                    vector = vector.flatten()
                
                # æ£€æŸ¥ç»´åº¦
                if len(vector) != VECTOR_DIMS:
                    continue
                
                # L2å½’ä¸€åŒ–
                norm = np.linalg.norm(vector)
                if norm > 0:
                    vector = vector / norm
                else:
                    continue
                
                # è½¬æ¢ä¸ºåˆ—è¡¨
                vector_list = vector.tolist()
                if len(vector_list) != VECTOR_DIMS:
                    continue
                
                # ç¡®ä¿æ˜¯floatç±»å‹
                vectors[orig_idx] = [float(x) for x in vector_list]
        
        except Exception as e:
            logger.warning(f"æ‰¹é‡å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            # å¦‚æœæ‰¹é‡å¤±è´¥ï¼Œå°è¯•é€ä¸ªç”Ÿæˆ
            for orig_idx, text in zip(batch_indices, batch_texts):
                vectors[orig_idx] = generate_vector(text)
    
    return vectors


def process_single_item(item, use_batch=False, vector_cache=None):
    """å¤„ç†å•æ¡æ•°æ®é¡¹ - ç”Ÿæˆæ ‡ç­¾ã€æè¿°ã€å®ä½“è¯çš„å‘é‡
    
    æ³¨æ„ï¼šESä¸­å­˜å‚¨çš„å­—æ®µåä¸æºæ–‡ä»¶ä¸€è‡´ï¼šentity_words_zh_vector å’Œ entity_words_en_vector
    è¿™äº›å­—æ®µå­˜å‚¨çš„æ˜¯NERæå–çš„å®ä½“è¯å‘é‡ã€‚
    
    Args:
        item: æ•°æ®é¡¹
        use_batch: æ˜¯å¦ä½¿ç”¨æ‰¹é‡å¤„ç†ï¼ˆéœ€è¦å¤–éƒ¨è°ƒç”¨æ‰¹é‡å‡½æ•°ï¼‰
        vector_cache: å‘é‡ç¼“å­˜å­—å…¸ï¼Œç”¨äºå­˜å‚¨æ‰¹é‡ç”Ÿæˆçš„å‘é‡
    """
    # é¦–æ¬¡è°ƒç”¨æ—¶æµ‹è¯•å‘é‡ç”Ÿæˆï¼ˆåªæµ‹è¯•ä¸€æ¬¡ï¼‰
    if not hasattr(process_single_item, '_tested'):
        test_vector = generate_vector("æµ‹è¯•")
        if test_vector:
            actual_dims = len(test_vector)
            if actual_dims != VECTOR_DIMS:
                logger.error(f"âŒ å‘é‡ç»´åº¦ä¸åŒ¹é…! æœŸæœ›: {VECTOR_DIMS}, å®é™…: {actual_dims}")
                logger.error(f"   è¯·æ£€æŸ¥æ¨¡å‹é…ç½®ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹ VECTOR_DIMS ä¸º {actual_dims}")
            else:
                logger.info(f"âœ… å‘é‡ç”Ÿæˆæµ‹è¯•é€šè¿‡: ç»´åº¦ = {actual_dims}")
        else:
            logger.error(f"âŒ å‘é‡ç”Ÿæˆæµ‹è¯•å¤±è´¥: generate_vector è¿”å› None")
        process_single_item._tested = True

    # æå–å…³é”®å­—æ®µ
    label = item.get("label", "")
    link = item.get("wikipedia") or item.get("wikipediaLink", "")
    aliases_en = item.get("en_aliases") or item.get("aliases_en", [])
    aliases_zh = item.get("zh_aliases") or item.get("aliases_zh", [])
    descriptions_en = item.get("en_description") or item.get("descriptions_en", "")
    descriptions_zh = item.get("zh_description") or item.get("descriptions_zh", "")
    content = item.get("content", "")
    
    # æå–å®ä½“è¯ï¼ˆä¼˜å…ˆä½¿ç”¨æ–°çš„å®ä½“è¯å­—æ®µï¼Œå…¼å®¹æ—§çš„é«˜é¢‘è¯å­—æ®µï¼‰
    # æ³¨æ„ï¼šç°åœ¨ä½¿ç”¨NERæ¨¡å‹æå–çš„å®ä½“è¯ï¼Œè€Œä¸æ˜¯é«˜é¢‘è¯ç»Ÿè®¡
    entity_words_zh = item.get("_entity_words_zh", item.get("_high_freq_words_zh", []))
    entity_words_en = item.get("_entity_words_en", item.get("_high_freq_words_en", []))
    
    # ä½¿ç”¨å®ä½“è¯åˆ—è¡¨ï¼ˆå˜é‡åä¿æŒç®€æ´ï¼‰
    entity_words_zh_list = entity_words_zh
    entity_words_en_list = entity_words_en
    
    # ä¼˜å…ˆä½¿ç”¨ find_top_k.py é¢„ç”Ÿæˆçš„å®ä½“è¯å‘é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    entity_words_zh_vector = item.get("_entity_words_zh_vector")
    entity_words_en_vector = item.get("_entity_words_en_vector")

    # æ„å»ºæ•°æ®å¯¹è±¡
    new_data = {
        "label": label,
        "link": link,
        "aliases_en": aliases_en if isinstance(aliases_en, list) else [],
        "aliases_zh": aliases_zh if isinstance(aliases_zh, list) else [],
        "descriptions_en": descriptions_en,
        "descriptions_zh": descriptions_zh,
        "content": content
    }
    
    # 1. ä¸ºæ ‡ç­¾ç”Ÿæˆå‘é‡ï¼ˆåŒ…å«æ‰€æœ‰åˆ«åï¼‰
    # 1.1 ä¸­æ–‡æ ‡ç­¾å‘é‡ï¼ˆlabel + æ‰€æœ‰ä¸­æ–‡åˆ«åï¼‰
    label_zh_parts = []
    if label:
        label_zh_parts.append(label)
    if aliases_zh and isinstance(aliases_zh, list):
        # åŒ…å«æ‰€æœ‰ä¸­æ–‡åˆ«åï¼Œä¸é™åˆ¶æ•°é‡
        label_zh_parts.extend(aliases_zh)
    
    if label_zh_parts:
        label_zh_text = " ".join(label_zh_parts)
        label_zh_vector = generate_vector(label_zh_text)
        if label_zh_vector and len(label_zh_vector) == VECTOR_DIMS:
            new_data["label_zh_vector"] = label_zh_vector
        else:
            logger.warning(f"ä¸­æ–‡æ ‡ç­¾å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}, åˆ«åæ•°: {len(aliases_zh) if aliases_zh else 0}")
    
    # 1.2 è‹±æ–‡æ ‡ç­¾å‘é‡ï¼ˆlabel + æ‰€æœ‰è‹±æ–‡åˆ«åï¼‰
    label_en_parts = []
    if label:
        label_en_parts.append(label)
    if aliases_en and isinstance(aliases_en, list):
        # åŒ…å«æ‰€æœ‰è‹±æ–‡åˆ«åï¼Œä¸é™åˆ¶æ•°é‡
        label_en_parts.extend(aliases_en)
    
    if label_en_parts:
        label_en_text = " ".join(label_en_parts)
        label_en_vector = generate_vector(label_en_text)
        if label_en_vector and len(label_en_vector) == VECTOR_DIMS:
            new_data["label_en_vector"] = label_en_vector
        else:
            logger.warning(f"è‹±æ–‡æ ‡ç­¾å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}, åˆ«åæ•°: {len(aliases_en) if aliases_en else 0}")
    
    # 1.3 é€šç”¨æ ‡ç­¾å‘é‡ï¼ˆä»…labelï¼Œç”¨äºå…¼å®¹æ€§ï¼‰
    if label:
        label_vector = generate_vector(label)
        if label_vector and len(label_vector) == VECTOR_DIMS:
            new_data["label_vector"] = label_vector
    
    # 2. ä¸ºä¸­æ–‡æè¿°ç”Ÿæˆå‘é‡
    if descriptions_zh and len(descriptions_zh.strip()) > 10:
        zh_desc_vector = generate_vector(descriptions_zh)
        if zh_desc_vector and len(zh_desc_vector) == VECTOR_DIMS:
            new_data["descriptions_zh_vector"] = zh_desc_vector
        else:
            logger.warning(f"ä¸­æ–‡æè¿°å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}")
    
    # 3. ä¸ºè‹±æ–‡æè¿°ç”Ÿæˆå‘é‡
    if descriptions_en and len(descriptions_en.strip()) > 10:
        en_desc_vector = generate_vector(descriptions_en)
        if en_desc_vector and len(en_desc_vector) == VECTOR_DIMS:
            new_data["descriptions_en_vector"] = en_desc_vector
        else:
            logger.warning(f"è‹±æ–‡æè¿°å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}")
    
    # 4. ä¸ºä¸­æ–‡å®ä½“è¯ç”Ÿæˆå‘é‡ï¼ˆä½¿ç”¨NERæå–çš„å®ä½“è¯ï¼‰
    # ä¼˜å…ˆä½¿ç”¨ find_top_k.py é¢„ç”Ÿæˆçš„å‘é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if entity_words_zh_vector and isinstance(entity_words_zh_vector, list) and len(entity_words_zh_vector) == VECTOR_DIMS:
        # ç›´æ¥ä½¿ç”¨é¢„ç”Ÿæˆçš„å‘é‡ï¼ˆå·²ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹å‘é‡åŒ–å¹¶åˆå¹¶ï¼‰
        new_data["entity_words_zh_vector"] = [float(x) for x in entity_words_zh_vector]
        logger.debug(f"ä½¿ç”¨é¢„ç”Ÿæˆçš„ä¸­æ–‡å®ä½“è¯å‘é‡ï¼Œæ ‡ç­¾: {label[:30]}, å®ä½“è¯æ•°: {len(entity_words_zh_list) if entity_words_zh_list else 0}")
    elif entity_words_zh_list and isinstance(entity_words_zh_list, list) and len(entity_words_zh_list) > 0:
        # å¦‚æœæ²¡æœ‰é¢„ç”Ÿæˆçš„å‘é‡ï¼Œåˆ™é‡æ–°å‘é‡åŒ–ï¼ˆä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ï¼‰
        # æ–¹æ³•ï¼šå¯¹æ¯ä¸ªå®ä½“è¯å•ç‹¬å‘é‡åŒ–ï¼Œç„¶ååˆå¹¶ï¼ˆä¸ find_top_k.py ä¿æŒä¸€è‡´ï¼‰
        try:
            from vector_model import batch_generate_vectors
            import numpy as np
            
            # æ‰¹é‡ç”Ÿæˆæ¯ä¸ªå®ä½“è¯çš„å‘é‡ï¼ˆä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ï¼‰
            entity_vectors = batch_generate_vectors(
                entity_words_zh_list,
                use_finetuned=True,
                target_dim=VECTOR_DIMS,
                batch_size=32
            )
            
            # è¿‡æ»¤æ‰Noneå€¼
            valid_vectors = [v for v in entity_vectors if v is not None and isinstance(v, list) and len(v) == VECTOR_DIMS]
            
            if valid_vectors:
                # åˆå¹¶å‘é‡ï¼ˆä½¿ç”¨å¹³å‡å€¼ï¼Œä¸ find_top_k.py ä¿æŒä¸€è‡´ï¼‰
                vectors_array = np.array(valid_vectors)
                merged_vector = np.mean(vectors_array, axis=0)
                
                # L2å½’ä¸€åŒ–
                norm = np.linalg.norm(merged_vector)
                if norm > 0:
                    merged_vector = merged_vector / norm
                    new_data["entity_words_zh_vector"] = [float(x) for x in merged_vector.tolist()]
                else:
                    logger.warning(f"ä¸­æ–‡å®ä½“è¯å‘é‡åˆå¹¶åå½’ä¸€åŒ–å¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}")
            else:
                logger.warning(f"ä¸­æ–‡å®ä½“è¯å‘é‡åŒ–å¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}, å®ä½“è¯æ•°: {len(entity_words_zh_list)}")
        except Exception as e:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•ï¼ˆå°†æ‰€æœ‰å®ä½“è¯ç”¨ç©ºæ ¼è¿æ¥ï¼Œç„¶åå‘é‡åŒ–ï¼‰
            logger.warning(f"æ‰¹é‡å‘é‡åŒ–å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ³•: {e}")
            zh_freq_text = " ".join(entity_words_zh_list)
            zh_freq_vector = generate_vector(zh_freq_text)
            if zh_freq_vector and len(zh_freq_vector) == VECTOR_DIMS:
                new_data["entity_words_zh_vector"] = zh_freq_vector
            else:
                logger.warning(f"ä¸­æ–‡å®ä½“è¯å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}, å®ä½“è¯æ•°: {len(entity_words_zh_list)}")
    
    # 5. ä¸ºè‹±æ–‡å®ä½“è¯ç”Ÿæˆå‘é‡ï¼ˆä½¿ç”¨NERæå–çš„å®ä½“è¯ï¼‰
    # ä¼˜å…ˆä½¿ç”¨ find_top_k.py é¢„ç”Ÿæˆçš„å‘é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if entity_words_en_vector and isinstance(entity_words_en_vector, list) and len(entity_words_en_vector) == VECTOR_DIMS:
        # ç›´æ¥ä½¿ç”¨é¢„ç”Ÿæˆçš„å‘é‡ï¼ˆå·²ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹å‘é‡åŒ–å¹¶åˆå¹¶ï¼‰
        new_data["entity_words_en_vector"] = [float(x) for x in entity_words_en_vector]
        logger.debug(f"ä½¿ç”¨é¢„ç”Ÿæˆçš„è‹±æ–‡å®ä½“è¯å‘é‡ï¼Œæ ‡ç­¾: {label[:30]}, å®ä½“è¯æ•°: {len(entity_words_en_list) if entity_words_en_list else 0}")
    elif entity_words_en_list and isinstance(entity_words_en_list, list) and len(entity_words_en_list) > 0:
        # å¦‚æœæ²¡æœ‰é¢„ç”Ÿæˆçš„å‘é‡ï¼Œåˆ™é‡æ–°å‘é‡åŒ–ï¼ˆä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ï¼‰
        try:
            from vector_model import batch_generate_vectors
            import numpy as np
            
            # æ‰¹é‡ç”Ÿæˆæ¯ä¸ªå®ä½“è¯çš„å‘é‡ï¼ˆä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ï¼‰
            entity_vectors = batch_generate_vectors(
                entity_words_en_list,
                use_finetuned=True,
                target_dim=VECTOR_DIMS,
                batch_size=32
            )
            
            # è¿‡æ»¤æ‰Noneå€¼
            valid_vectors = [v for v in entity_vectors if v is not None and isinstance(v, list) and len(v) == VECTOR_DIMS]
            
            if valid_vectors:
                # åˆå¹¶å‘é‡ï¼ˆä½¿ç”¨å¹³å‡å€¼ï¼Œä¸ find_top_k.py ä¿æŒä¸€è‡´ï¼‰
                vectors_array = np.array(valid_vectors)
                merged_vector = np.mean(vectors_array, axis=0)
                
                # L2å½’ä¸€åŒ–
                norm = np.linalg.norm(merged_vector)
                if norm > 0:
                    merged_vector = merged_vector / norm
                    new_data["entity_words_en_vector"] = [float(x) for x in merged_vector.tolist()]
                else:
                    logger.warning(f"è‹±æ–‡å®ä½“è¯å‘é‡åˆå¹¶åå½’ä¸€åŒ–å¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}")
            else:
                logger.warning(f"è‹±æ–‡å®ä½“è¯å‘é‡åŒ–å¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}, å®ä½“è¯æ•°: {len(entity_words_en_list)}")
        except Exception as e:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•ï¼ˆå°†æ‰€æœ‰å®ä½“è¯ç”¨ç©ºæ ¼è¿æ¥ï¼Œç„¶åå‘é‡åŒ–ï¼‰
            logger.warning(f"æ‰¹é‡å‘é‡åŒ–å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ³•: {e}")
            en_freq_text = " ".join(entity_words_en_list)
            en_freq_vector = generate_vector(en_freq_text)
            if en_freq_vector and len(en_freq_vector) == VECTOR_DIMS:
                new_data["entity_words_en_vector"] = en_freq_vector
            else:
                logger.warning(f"è‹±æ–‡å®ä½“è¯å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œæ ‡ç­¾: {label[:30]}, å®ä½“è¯æ•°: {len(entity_words_en_list)}")

    return new_data


def count_lines(filename):
    """å¿«é€Ÿè®¡ç®—æ–‡ä»¶è¡Œæ•°"""
    with open(filename, 'r', encoding='utf-8') as f:
        return sum(1 for _ in f)


def process_and_import_to_es(input_path, batch_size=20, request_timeout=120, vector_batch_size=32, use_batch_vectors=True):
    """å¤„ç†JSONLæ–‡ä»¶å¹¶å¯¼å…¥åˆ°ES - ä¼˜åŒ–æ€§èƒ½
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        batch_size: ESæ‰¹é‡å¯¼å…¥å¤§å°
        request_timeout: ESè¯·æ±‚è¶…æ—¶æ—¶é—´
        vector_batch_size: å‘é‡æ‰¹é‡ç”Ÿæˆå¤§å°ï¼ˆGPUåŠ é€Ÿæ—¶å»ºè®®32-64ï¼‰
        use_batch_vectors: æ˜¯å¦ä½¿ç”¨æ‰¹é‡å‘é‡ç”Ÿæˆï¼ˆæ˜¾è‘—æå‡é€Ÿåº¦ï¼‰
    """
    print("=" * 60)
    print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_path}")
    print("=" * 60)
    
    # å‰ç½®æ£€æŸ¥
    print("\n[å‰ç½®æ£€æŸ¥]")
    if not os.path.exists(input_path):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    try:
        total_lines = count_lines(input_path)
        print(f"âœ… æ–‡ä»¶å­˜åœ¨ï¼Œæ€»è¡Œæ•°: {total_lines}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶: {e}")
        return
    
    # æ£€æŸ¥ESè¿æ¥
    try:
        if not es.indices.exists(index=INDEX_NAME):
            print(f"âŒ é”™è¯¯: ç´¢å¼• {INDEX_NAME} ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºç´¢å¼•")
            return
        print(f"âœ… ESç´¢å¼•å­˜åœ¨: {INDEX_NAME}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: ESè¿æ¥å¤±è´¥: {e}")
        return
    
    # æ£€æŸ¥è®¾å¤‡
    device_info = "GPU" if torch.cuda.is_available() else "CPU"
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device_info}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        # GPUæ¨¡å¼ä¸‹å»ºè®®ä½¿ç”¨æ‰¹é‡å‘é‡ç”Ÿæˆ
        if not use_batch_vectors:
            print(f"   âš ï¸  å»ºè®®å¯ç”¨æ‰¹é‡å‘é‡ç”Ÿæˆä»¥æå‡é€Ÿåº¦ï¼ˆå½“å‰å·²ç¦ç”¨ï¼‰")
    else:
        # CPUæ¨¡å¼ä¸‹æ‰¹é‡å¤„ç†å¯èƒ½è¾ƒæ…¢ï¼Œä½†ä»æœ‰æå‡
        if use_batch_vectors:
            print(f"   ğŸ’¡ CPUæ¨¡å¼ï¼šæ‰¹é‡å‘é‡ç”Ÿæˆå¯èƒ½æå‡æœ‰é™")
    
    if use_batch_vectors:
        print(f"âœ… æ‰¹é‡å‘é‡ç”Ÿæˆ: å¯ç”¨ (æ‰¹é‡å¤§å°: {vector_batch_size})")
    else:
        print(f"âš ï¸  æ‰¹é‡å‘é‡ç”Ÿæˆ: ç¦ç”¨ (é€ä¸ªç”Ÿæˆ)")
    print("=" * 60)
    
    actions = []
    total_imported = 0
    failed_count = 0
    vector_stats = {
        'label': 0,
        'label_zh': 0,
        'label_en': 0,
        'descriptions_zh': 0,
        'descriptions_en': 0,
        'entity_words_zh': 0,
        'entity_words_en': 0
    }
    start_time = time.time()
    last_speed_time = start_time
    last_speed_count = 0

    # è®°å½•å¯¼å…¥å‰çš„æ–‡æ¡£æ•°é‡
    try:
        doc_count_before = es.count(index=INDEX_NAME)["count"]
        print(f"\nå¯¼å…¥å‰ç´¢å¼•æ–‡æ¡£æ•°é‡: {doc_count_before}")
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: è·å–åˆå§‹æ–‡æ¡£æ•°å¤±è´¥: {e}")
        doc_count_before = 0

    with open(input_path, 'r', encoding='utf-8') as f:
        progress_bar = tqdm(total=total_lines, desc="å¤„ç†è¿›åº¦", unit="æ¡")

        for line_num, line in enumerate(f, 1):
            try:
                if not line.strip():
                    progress_bar.update(1)
                    continue

                data = json.loads(line.strip())
                
                # å¦‚æœä½¿ç”¨æ‰¹é‡å‘é‡ç”Ÿæˆï¼Œå…ˆæ”¶é›†æ–‡æœ¬ï¼Œç¨åæ‰¹é‡å¤„ç†
                if use_batch_vectors and line_num % (batch_size * 2) == 1:
                    # æ¯å¤„ç†ä¸€å®šæ•°é‡åï¼Œæ‰¹é‡ç”Ÿæˆå‘é‡
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä»ç„¶é€ä¸ªç”Ÿæˆï¼Œä½†å¯ä»¥ä¼˜åŒ–ä¸ºçœŸæ­£çš„æ‰¹é‡
                    transformed_data = process_single_item(data)
                else:
                    transformed_data = process_single_item(data)

                # ç»Ÿè®¡å‘é‡æ•°é‡
                if 'label_vector' in transformed_data:
                    vector_stats['label'] += 1
                if 'label_zh_vector' in transformed_data:
                    vector_stats['label_zh'] += 1
                if 'label_en_vector' in transformed_data:
                    vector_stats['label_en'] += 1
                if 'descriptions_zh_vector' in transformed_data:
                    vector_stats['descriptions_zh'] += 1
                if 'descriptions_en_vector' in transformed_data:
                    vector_stats['descriptions_en'] += 1
                if 'entity_words_zh_vector' in transformed_data:
                    vector_stats['entity_words_zh'] += 1
                if 'entity_words_en_vector' in transformed_data:
                    vector_stats['entity_words_en'] += 1

                # åœ¨å¼€å§‹å¤„ç†æ—¶æ˜¾ç¤ºä¿¡æ¯
                if line_num == 1:
                    print(f"\n[ç¬¬ä¸€æ¡æ•°æ®ç¤ºä¾‹]")
                    label = transformed_data.get('label', 'N/A')
                    aliases_zh = transformed_data.get('aliases_zh', [])
                    aliases_en = transformed_data.get('aliases_en', [])
                    print(f"  æ ‡ç­¾: {label}")
                    print(f"  ä¸­æ–‡åˆ«åæ•°: {len(aliases_zh) if isinstance(aliases_zh, list) else 0}")
                    print(f"  è‹±æ–‡åˆ«åæ•°: {len(aliases_en) if isinstance(aliases_en, list) else 0}")
                    print(f"  æ ‡ç­¾å‘é‡: {'âœ“' if 'label_vector' in transformed_data else 'âœ—'}")
                    print(f"  ä¸­æ–‡æ ‡ç­¾å‘é‡(å«æ‰€æœ‰åˆ«å): {'âœ“' if 'label_zh_vector' in transformed_data else 'âœ—'}")
                    print(f"  è‹±æ–‡æ ‡ç­¾å‘é‡(å«æ‰€æœ‰åˆ«å): {'âœ“' if 'label_en_vector' in transformed_data else 'âœ—'}")
                    print(f"  ä¸­æ–‡æè¿°å‘é‡: {'âœ“' if 'descriptions_zh_vector' in transformed_data else 'âœ—'}")
                    print(f"  è‹±æ–‡æè¿°å‘é‡: {'âœ“' if 'descriptions_en_vector' in transformed_data else 'âœ—'}")
                    # æ³¨æ„ï¼šå®ä½“è¯åˆ—è¡¨ä¸ä¼šå­˜å…¥ESï¼Œåªå­˜å‚¨å‘é‡ï¼Œæ‰€ä»¥ä»åŸå§‹æ•°æ®è¯»å–ç”¨äºæ˜¾ç¤º
                    entity_words_zh = data.get('_entity_words_zh', data.get('_high_freq_words_zh', []))
                    entity_words_en = data.get('_entity_words_en', data.get('_high_freq_words_en', []))
                    print(f"  ä¸­æ–‡å®ä½“è¯æ•°: {len(entity_words_zh) if isinstance(entity_words_zh, list) else 0}")
                    print(f"  è‹±æ–‡å®ä½“è¯æ•°: {len(entity_words_en) if isinstance(entity_words_en, list) else 0}")
                    # ESä¸­å­˜å‚¨çš„å­—æ®µåä¸æºæ–‡ä»¶ä¸€è‡´ï¼šentity_words_*_vector
                    print(f"  ä¸­æ–‡å®ä½“è¯å‘é‡(ESå­—æ®µ: entity_words_zh_vector): {'âœ“' if 'entity_words_zh_vector' in transformed_data else 'âœ—'}")
                    print(f"  è‹±æ–‡å®ä½“è¯å‘é‡(ESå­—æ®µ: entity_words_en_vector): {'âœ“' if 'entity_words_en_vector' in transformed_data else 'âœ—'}")
                    print()

                actions.append({
                    "_index": INDEX_NAME,
                    "_source": transformed_data
                })

                # æ‰¹é‡å¯¼å…¥ - ä½¿ç”¨æ–°çš„APIè°ƒç”¨æ–¹å¼
                if len(actions) >= batch_size:
                    try:
                        # ä¿®æ­£ï¼šä½¿ç”¨æ–°çš„APIè°ƒç”¨æ–¹å¼
                        success, failed = helpers.bulk(
                            es.options(request_timeout=request_timeout),
                            actions,
                            raise_on_error=False,
                            stats_only=False
                        )

                        if failed:
                            error_count = len(failed)
                            failed_count += error_count
                            total_imported += (len(actions) - error_count)
                            # è®°å½•é”™è¯¯ä¿¡æ¯
                            if failed:
                                error_info = failed[0].get('index', {}).get('error', {})
                                logger.warning(
                                    f"æ‰¹é‡å¯¼å…¥éƒ¨åˆ†å¤±è´¥: {error_count}æ¡å¤±è´¥ï¼Œé”™è¯¯: {error_info.get('reason', 'æœªçŸ¥é”™è¯¯')[:100]}")
                        else:
                            total_imported += len(actions)
                            logger.debug(f"æˆåŠŸå¯¼å…¥ {len(actions)} æ¡æ•°æ®åˆ°ES")

                        actions = []

                        # æ¯500æ¡æ›´æ–°ä¸€æ¬¡è¿›åº¦æ¡
                        if total_imported % 500 == 0:
                            current_time = time.time()
                            elapsed = current_time - start_time
                            avg_speed = total_imported / elapsed if elapsed > 0 else 0

                            # è®¡ç®—æœ€è¿‘ä¸€æ®µæ—¶é—´çš„é€Ÿåº¦
                            recent_elapsed = current_time - last_speed_time
                            recent_speed = (total_imported - last_speed_count) / recent_elapsed if recent_elapsed > 0 else 0
                            last_speed_time = current_time
                            last_speed_count = total_imported

                            total_vectors = sum(vector_stats.values())
                            progress_bar.set_postfix({
                                'å·²å¯¼å…¥': total_imported,
                                'å‘é‡': total_vectors,
                                'å¤±è´¥': failed_count,
                                'é€Ÿåº¦': f'{recent_speed:.1f}æ¡/s',
                                'è®¾å¤‡': device_info
                            })

                    except Exception as e:
                        error_msg = str(e)[:200]
                        logger.error(f"æ‰¹é‡å¯¼å…¥å¼‚å¸¸: {error_msg}")
                        print(f"âŒ æ‰¹é‡å¯¼å…¥é”™è¯¯: {error_msg}")
                        failed_count += len(actions)
                        actions = []

                progress_bar.update(1)

            except Exception as e:
                failed_count += 1
                progress_bar.update(1)
                error_msg = str(e)[:200]
                if failed_count <= 10:
                    print(f"âŒ ç¬¬{line_num}è¡Œå¤„ç†å¤±è´¥: {error_msg}")
                logger.warning(f"ç¬¬{line_num}è¡Œå¤„ç†å¤±è´¥: {error_msg}")

        # å¯¼å…¥å‰©ä½™æ•°æ®
        if actions:
            try:
                success, failed = helpers.bulk(
                    es.options(request_timeout=request_timeout),
                    actions,
                    raise_on_error=False,
                    stats_only=False
                )

                if failed:
                    error_count = len(failed)
                    failed_count += error_count
                    total_imported += (len(actions) - error_count)
                else:
                    total_imported += len(actions)
            except Exception as e:
                logger.error(f"æœ€åä¸€æ‰¹å¯¼å…¥å¼‚å¸¸: {e}")
                failed_count += len(actions)

        progress_bar.close()

    elapsed_time = time.time() - start_time
    total_vectors = sum(vector_stats.values())

    print(f"\n{'=' * 60}")
    print(f"å¯¼å…¥å®Œæˆ!")
    print(f"{'=' * 60}")
    print(f"æˆåŠŸå¯¼å…¥: {total_imported}æ¡")
    print(f"å¤±è´¥: {failed_count}æ¡")
    print(f"æ€»è€—æ—¶: {elapsed_time:.1f}ç§’ ({elapsed_time / 60:.1f}åˆ†é’Ÿ)")
    avg_speed = total_imported / elapsed_time if elapsed_time > 0 else 0
    print(f"å¹³å‡é€Ÿåº¦: {avg_speed:.2f}æ¡/ç§’")
    print(f"\nå‘é‡ç”Ÿæˆç»Ÿè®¡:")
    print(f"  æ ‡ç­¾å‘é‡(ä»…label): {vector_stats['label']}ä¸ª")
    print(f"  ä¸­æ–‡æ ‡ç­¾å‘é‡(label+æ‰€æœ‰ä¸­æ–‡åˆ«å): {vector_stats['label_zh']}ä¸ª")
    print(f"  è‹±æ–‡æ ‡ç­¾å‘é‡(label+æ‰€æœ‰è‹±æ–‡åˆ«å): {vector_stats['label_en']}ä¸ª")
    print(f"  ä¸­æ–‡æè¿°å‘é‡: {vector_stats['descriptions_zh']}ä¸ª")
    print(f"  è‹±æ–‡æè¿°å‘é‡: {vector_stats['descriptions_en']}ä¸ª")
    print(f"  ä¸­æ–‡å®ä½“è¯å‘é‡: {vector_stats['entity_words_zh']}ä¸ª")
    print(f"  è‹±æ–‡å®ä½“è¯å‘é‡: {vector_stats['entity_words_en']}ä¸ª")
    print(f"  å‘é‡æ€»æ•°: {total_vectors}ä¸ª")
    
    if total_imported > 0:
        print(f"\nå‘é‡ç”Ÿæˆç‡:")
        print(f"  æ ‡ç­¾(ä»…label): {vector_stats['label']/total_imported*100:.1f}%")
        print(f"  ä¸­æ–‡æ ‡ç­¾(label+æ‰€æœ‰åˆ«å): {vector_stats['label_zh']/total_imported*100:.1f}%")
        print(f"  è‹±æ–‡æ ‡ç­¾(label+æ‰€æœ‰åˆ«å): {vector_stats['label_en']/total_imported*100:.1f}%")
        print(f"  ä¸­æ–‡æè¿°: {vector_stats['descriptions_zh']/total_imported*100:.1f}%")
        print(f"  è‹±æ–‡æè¿°: {vector_stats['descriptions_en']/total_imported*100:.1f}%")
        print(f"  ä¸­æ–‡å®ä½“è¯å‘é‡: {vector_stats['entity_words_zh']/total_imported*100:.1f}%")
        print(f"  è‹±æ–‡å®ä½“è¯å‘é‡: {vector_stats['entity_words_en']/total_imported*100:.1f}%")

    # æ˜¾ç¤ºè®¾å¤‡ä½¿ç”¨æƒ…å†µ
    print(f"\nè®¾å¤‡ä¿¡æ¯:")
    print(f"  ä½¿ç”¨è®¾å¤‡: {device_info}")
    if torch.cuda.is_available():
        gpu_memory_used = torch.cuda.max_memory_allocated() / 1024 ** 3  # GB
        print(f"  GPUæ˜¾å­˜ä½¿ç”¨: {gpu_memory_used:.2f} GB")
    print(f"{'=' * 60}")

    # è·å–æœ€ç»ˆç»Ÿè®¡
    try:
        doc_count_after = es.count(index=INDEX_NAME)["count"]
        actual_imported = doc_count_after - doc_count_before
        print(f"å®é™…æ–°å¢æ–‡æ¡£æ•°: {actual_imported}")
        print(f"å¯¼å…¥åç´¢å¼•ä¸­æ–‡æ¡£æ€»æ•°: {doc_count_after}")

        # ä½¿ç”¨ exists æŸ¥è¯¢æ£€æŸ¥å‘é‡å­—æ®µæ˜¯å¦å­˜åœ¨ï¼ˆæ›´å¯é ï¼‰
        print("\n[å‘é‡å­—æ®µå­˜åœ¨æ€§æ£€æŸ¥ - ä½¿ç”¨ exists æŸ¥è¯¢]")
        
        vector_fields = [
            'label_vector',
            'label_zh_vector',
            'label_en_vector',
            'descriptions_zh_vector',
            'descriptions_en_vector',
            'entity_words_zh_vector',
            'entity_words_en_vector'
        ]
        
        field_stats = {}
        for field in vector_fields:
            try:
                # ä½¿ç”¨ exists æŸ¥è¯¢ç»Ÿè®¡æœ‰å¤šå°‘æ–‡æ¡£åŒ…å«è¯¥å­—æ®µ
                exists_query = {
                    "query": {
                        "exists": {
                            "field": field
                        }
                    }
                }
                result = es.count(index=INDEX_NAME, body=exists_query)
                count = result.get('count', 0)
                field_stats[field] = count
                percentage = (count / doc_count_after * 100) if doc_count_after > 0 else 0
                print(f"  {field}: {count} ä¸ªæ–‡æ¡£ ({percentage:.1f}%)")
            except Exception as e:
                print(f"  {field}: æ£€æŸ¥å¤±è´¥ - {e}")
                field_stats[field] = 0
        
        # æ£€æŸ¥è‡³å°‘æœ‰ä¸€ä¸ªå‘é‡å­—æ®µçš„æ–‡æ¡£æ•°
        try:
            any_vector_query = {
                "query": {
                    "bool": {
                        "should": [
                            {"exists": {"field": field}} for field in vector_fields
                        ],
                        "minimum_should_match": 1
                    }
                }
            }
            any_vector_result = es.count(index=INDEX_NAME, body=any_vector_query)
            any_vector_count = any_vector_result.get('count', 0)
            any_vector_percentage = (any_vector_count / doc_count_after * 100) if doc_count_after > 0 else 0
            print(f"\nè‡³å°‘æœ‰ä¸€ä¸ªå‘é‡å­—æ®µçš„æ–‡æ¡£æ•°: {any_vector_count}/{doc_count_after} ({any_vector_percentage:.1f}%)")
        except Exception as e:
            print(f"\næ£€æŸ¥è‡³å°‘æœ‰ä¸€ä¸ªå‘é‡å­—æ®µçš„æ–‡æ¡£æ•°å¤±è´¥: {e}")
        
        # éšæœºé‡‡æ ·å‡ ä¸ªæ–‡æ¡£ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„å‘é‡å­—æ®µæƒ…å†µ
        print("\n[éšæœºé‡‡æ ·æ–‡æ¡£æ£€æŸ¥ - æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£åŒ…å«çš„å‘é‡ç±»å‹]")
        print("è¯´æ˜: æ˜¾ç¤ºéšæœºæŠ½å–çš„5ä¸ªæ–‡æ¡£ï¼Œåˆ—å‡ºæ¯ä¸ªæ–‡æ¡£åŒ…å«å“ªäº›å‘é‡å­—æ®µ")
        try:
            sample_query = {
                "size": 5,
                "_source": ["label"] + vector_fields,  # åŒæ—¶è·å–labelå’Œæ‰€æœ‰å‘é‡å­—æ®µ
                "query": {
                    "match_all": {}
                }
            }
            sample_result = es.search(index=INDEX_NAME, body=sample_query)
            
            for hit in sample_result['hits']['hits']:
                source = hit['_source']
                label = source.get('label', 'N/A')
                
                # æ£€æŸ¥æ¯ä¸ªå‘é‡å­—æ®µæ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ exists æŸ¥è¯¢éªŒè¯ï¼ŒåŒæ—¶æ£€æŸ¥ _sourceï¼‰
                doc_vectors = {}
                for field in vector_fields:
                    # å…ˆæ£€æŸ¥ _source ä¸­æ˜¯å¦æœ‰è¯¥å­—æ®µ
                    has_in_source = field in source
                    # å¦‚æœ _source ä¸­æ²¡æœ‰ï¼Œå†ç”¨ exists æŸ¥è¯¢ç¡®è®¤
                    if not has_in_source:
                        try:
                            exists_check = {
                                "query": {
                                    "bool": {
                                        "must": [
                                            {"term": {"_id": hit['_id']}},
                                            {"exists": {"field": field}}
                                        ]
                                    }
                                }
                            }
                            check_result = es.count(index=INDEX_NAME, body=exists_check)
                            has_in_source = check_result.get('count', 0) > 0
                        except:
                            pass
                    doc_vectors[field] = has_in_source
                
                # æ˜¾ç¤ºå‘é‡å­—æ®µçŠ¶æ€ï¼ˆä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼ï¼‰
                vector_status_parts = []
                if doc_vectors.get('label_vector'): 
                    vector_status_parts.append('âœ“æ ‡ç­¾')
                if doc_vectors.get('label_zh_vector'): 
                    vector_status_parts.append('âœ“ä¸­æ–‡æ ‡ç­¾')
                if doc_vectors.get('label_en_vector'): 
                    vector_status_parts.append('âœ“è‹±æ–‡æ ‡ç­¾')
                if doc_vectors.get('descriptions_zh_vector'): 
                    vector_status_parts.append('âœ“ä¸­æ–‡æè¿°')
                if doc_vectors.get('descriptions_en_vector'): 
                    vector_status_parts.append('âœ“è‹±æ–‡æè¿°')
                if doc_vectors.get('entity_words_zh_vector'): 
                    vector_status_parts.append('âœ“ä¸­æ–‡å®ä½“è¯')
                if doc_vectors.get('entity_words_en_vector'): 
                    vector_status_parts.append('âœ“è‹±æ–‡å®ä½“è¯')
                
                # æ˜¾ç¤ºç¼ºå¤±çš„å­—æ®µ
                missing_parts = []
                if not doc_vectors.get('label_vector'): 
                    missing_parts.append('âœ—æ ‡ç­¾')
                if not doc_vectors.get('label_zh_vector'): 
                    missing_parts.append('âœ—ä¸­æ–‡æ ‡ç­¾')
                if not doc_vectors.get('label_en_vector'): 
                    missing_parts.append('âœ—è‹±æ–‡æ ‡ç­¾')
                if not doc_vectors.get('descriptions_zh_vector'): 
                    missing_parts.append('âœ—ä¸­æ–‡æè¿°')
                if not doc_vectors.get('descriptions_en_vector'): 
                    missing_parts.append('âœ—è‹±æ–‡æè¿°')
                if not doc_vectors.get('entity_words_zh_vector'): 
                    missing_parts.append('âœ—ä¸­æ–‡å®ä½“è¯')
                if not doc_vectors.get('entity_words_en_vector'): 
                    missing_parts.append('âœ—è‹±æ–‡å®ä½“è¯')
                
                # æ˜¾ç¤ºç»“æœ
                if vector_status_parts:
                    status_str = " | ".join(vector_status_parts)
                    if missing_parts:
                        status_str += f" | ç¼ºå¤±: {', '.join(missing_parts)}"
                    print(f"  {label[:30]}: {status_str}")
                else:
                    print(f"  {label[:30]}: æ— å‘é‡å­—æ®µ")
        except Exception as e:
            print(f"é‡‡æ ·æ£€æŸ¥å¤±è´¥: {e}")
            logger.error(f"é‡‡æ ·æ£€æŸ¥å¤±è´¥è¯¦æƒ…: {e}")

    except Exception as e:
        print(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥è¯¦æƒ…: {e}")


if __name__ == "__main__":
    import os
    import sys

    print("=" * 60)
    print("å¼€å§‹å‘é‡åŒ–å¯¼å…¥æµç¨‹")
    print("=" * 60)

    # é»˜è®¤å¤„ç†top_k_zh.jsonlå’Œtop_k_en.jsonlï¼ˆé¢„å¤„ç†åçš„é«˜é¢‘å®ä½“æ–‡ä»¶ï¼‰
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šå•ä¸ªæ–‡ä»¶
    target_file = None
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--file" or sys.argv[1] == "-f":
            # ä½¿ç”¨--fileå‚æ•°æŒ‡å®šæ–‡ä»¶
            if len(sys.argv) > 2:
                target_file = sys.argv[2]
            else:
                print("é”™è¯¯: --fileå‚æ•°éœ€è¦æŒ‡å®šæ–‡ä»¶è·¯å¾„")
                exit(1)
        else:
            # å…¶ä»–å‚æ•°ä½œä¸ºæ–‡ä»¶è·¯å¾„
            target_file = sys.argv[1]
    
    # å…ˆåˆ›å»ºæ­£ç¡®çš„ç´¢å¼•æ˜ å°„
    print("åˆ›å»ºå‘é‡ç´¢å¼•æ˜ å°„...")
    if not create_vector_index():
        print("ç´¢å¼•åˆ›å»ºå¤±è´¥ï¼Œé€€å‡º")
        exit(1)

    processed_files = []
    vector_batch = 64 if torch.cuda.is_available() else 16
    
    # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶è·¯å¾„ï¼Œåªå¤„ç†è¯¥æ–‡ä»¶
    if target_file:
        if os.path.exists(target_file):
            print(f"\nå¤„ç†æŒ‡å®šæ–‡ä»¶: {target_file}")
            process_and_import_to_es(target_file, batch_size=20, request_timeout=180,
                                    vector_batch_size=vector_batch, use_batch_vectors=True)
            processed_files.append(target_file)
        else:
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {target_file}")
            exit(1)
    
    # é»˜è®¤å¤„ç†entity_words_zh.jsonlå’Œentity_words_en.jsonlï¼ˆé¢„å¤„ç†åçš„å®ä½“è¯æ–‡ä»¶ï¼‰
    else:
        # æŸ¥æ‰¾é¢„å¤„ç†åçš„å®ä½“è¯æ–‡ä»¶ï¼ˆç”± find_top_k.py ç”Ÿæˆï¼‰
        # ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
        if ENTITY_WORDS_ZH_FILE and os.path.exists(ENTITY_WORDS_ZH_FILE):
            target_files = [ENTITY_WORDS_ZH_FILE]
        elif ENTITY_WORDS_EN_FILE and os.path.exists(ENTITY_WORDS_EN_FILE):
            target_files = [ENTITY_WORDS_EN_FILE]
        else:
            target_files = [
                "entity_words_zh.jsonl",  # ä¸­æ–‡æ–‡ä»¶å¤„ç†ç»“æœï¼ˆNERæå–çš„å®ä½“è¯ï¼‰
                "entity_words_en.jsonl"   # è‹±æ–‡æ–‡ä»¶å¤„ç†ç»“æœï¼ˆNERæå–çš„å®ä½“è¯ï¼‰
            ]
        
        found_files = []
        for target_file in target_files:
            # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾æ–‡ä»¶ï¼šå½“å‰ç›®å½• -> dataç›®å½• -> çˆ¶ç›®å½•
            if os.path.exists(target_file):
                found_files.append(target_file)
                continue
            
            # å°è¯•åœ¨ data ç›®å½•æŸ¥æ‰¾ï¼ˆfind_top_k.py çš„è¾“å‡ºç›®å½•ï¼‰
            data_dir = os.path.join(parent_dir, 'data')
            data_target = os.path.join(data_dir, target_file)
            if os.path.exists(data_target):
                found_files.append(data_target)
                continue
            
            # å°è¯•åœ¨çˆ¶ç›®å½•æŸ¥æ‰¾
            parent_target = os.path.join(parent_dir, target_file)
            if os.path.exists(parent_target):
                found_files.append(parent_target)
        
        if found_files:
            for target_file in found_files:
                file_name = os.path.basename(target_file)
                file_type = "ä¸­æ–‡" if "zh" in file_name else "è‹±æ–‡"
                print(f"\næ‰¾åˆ°é¢„å¤„ç†åçš„{file_type}å®ä½“è¯æ–‡ä»¶: {target_file}")
                print(f"å¼€å§‹å¤„ç†: {target_file}")
                process_and_import_to_es(target_file, batch_size=20, request_timeout=180,
                                        vector_batch_size=vector_batch, use_batch_vectors=True)
                processed_files.append(target_file)
        else:
            print("âŒ é”™è¯¯: æœªæ‰¾åˆ°é¢„å¤„ç†åçš„å®ä½“è¯æ–‡ä»¶")
            print("   æœŸæœ›çš„æ–‡ä»¶:")
            print("     - entity_words_zh.jsonl (ä¸­æ–‡æ–‡ä»¶å¤„ç†ç»“æœï¼ŒåŒ…å«NERæå–çš„å®ä½“è¯)")
            print("     - entity_words_en.jsonl (è‹±æ–‡æ–‡ä»¶å¤„ç†ç»“æœï¼ŒåŒ…å«NERæå–çš„å®ä½“è¯)")
            print("\n   è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆé¢„å¤„ç†æ–‡ä»¶:")
            print("   cd data")
            print("   python find_top_k.py")
            print("\n   æˆ–è€…:")
            print("   python data/find_top_k.py")
            exit(1)

    if not processed_files:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶")
        exit(1)

    print(f"\nå·²å®Œæˆå¤„ç†ä»¥ä¸‹æ–‡ä»¶:")
    for f in processed_files:
        print(f"  - {f}")

    print("\nå¯¼å…¥æµç¨‹å®Œæˆ! è¯·æ£€æŸ¥é˜¿é‡Œäº‘æ§åˆ¶å°çš„å‘é‡å­˜å‚¨ç”¨é‡")