#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ•°æ®æ–‡ä»¶æ ¼å¼æ ¡éªŒè„šæœ¬
æ£€æŸ¥å¾…å¤„ç†çš„ JSONL æ–‡ä»¶æ˜¯å¦ç¬¦åˆå‘é‡ç”Ÿæˆçš„è¦æ±‚
"""

import json
import os
from collections import defaultdict

def validate_jsonl_file(file_path, sample_size=100):
    """
    æ ¡éªŒ JSONL æ–‡ä»¶æ ¼å¼
    
    Args:
        file_path: JSONL æ–‡ä»¶è·¯å¾„
        sample_size: æŠ½æ ·æ£€æŸ¥çš„æ–‡æ¡£æ•°é‡
    """
    print("=" * 70)
    print(f"å¼€å§‹æ ¡éªŒæ•°æ®æ–‡ä»¶: {file_path}")
    print("=" * 70)
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_lines = 0
    valid_lines = 0
    invalid_lines = 0
    
    # å­—æ®µç»Ÿè®¡
    field_stats = defaultdict(int)
    missing_fields = defaultdict(int)
    
    # å‘é‡ç”Ÿæˆæ‰€éœ€å­—æ®µ
    required_fields_for_vector = {
        'zh': ['label', 'zh_description', 'zh_aliases'],
        'en': ['label', 'en_description', 'en_aliases']
    }
    
    # é”™è¯¯è¯¦æƒ…
    errors = []
    warnings = []
    
    # æ ·æœ¬æ–‡æ¡£
    sample_docs = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            total_lines += 1
            
            # è·³è¿‡ç©ºè¡Œ
            if not line.strip():
                continue
            
            try:
                # è§£æ JSON
                data = json.loads(line.strip())
                valid_lines += 1
                
                # æ”¶é›†å­—æ®µ
                for key in data.keys():
                    field_stats[key] += 1
                
                # æ£€æŸ¥å¿…è¦å­—æ®µ
                label = data.get("label", "")
                descriptions_zh = data.get("zh_description") or data.get("descriptions_zh", "")
                descriptions_en = data.get("en_description") or data.get("descriptions_en", "")
                aliases_zh = data.get("zh_aliases") or data.get("aliases_zh", [])
                aliases_en = data.get("en_aliases") or data.get("aliases_en", [])
                
                # æ£€æŸ¥ä¸­æ–‡å‘é‡ç”Ÿæˆæ¡ä»¶
                can_generate_zh_vector = False
                if descriptions_zh and len(descriptions_zh.strip()) > 10:
                    can_generate_zh_vector = True
                elif label:
                    can_generate_zh_vector = True
                
                # æ£€æŸ¥è‹±æ–‡å‘é‡ç”Ÿæˆæ¡ä»¶
                can_generate_en_vector = False
                if descriptions_en and len(descriptions_en.strip()) > 10:
                    can_generate_en_vector = True
                elif label:
                    can_generate_en_vector = True
                
                # æ£€æŸ¥ label_vector ç”Ÿæˆæ¡ä»¶
                can_generate_label_zh_vector = bool(label)
                can_generate_label_en_vector = bool(label)
                
                # è®°å½•ç¼ºå¤±å­—æ®µ
                if not label:
                    missing_fields['label'] += 1
                if not descriptions_zh and not descriptions_zh:
                    missing_fields['descriptions_zh'] += 1
                if not descriptions_en and not descriptions_en:
                    missing_fields['descriptions_en'] += 1
                
                # æ”¶é›†æ ·æœ¬æ–‡æ¡£ï¼ˆå‰ sample_size ä¸ªï¼‰
                if len(sample_docs) < sample_size:
                    sample_docs.append({
                        'line_num': line_num,
                        'label': label,
                        'has_descriptions_zh': bool(descriptions_zh and len(descriptions_zh.strip()) > 10),
                        'has_descriptions_en': bool(descriptions_en and len(descriptions_en.strip()) > 10),
                        'has_aliases_zh': bool(aliases_zh and isinstance(aliases_zh, list) and len(aliases_zh) > 0),
                        'has_aliases_en': bool(aliases_en and isinstance(aliases_en, list) and len(aliases_en) > 0),
                        'can_generate_zh_vector': can_generate_zh_vector,
                        'can_generate_en_vector': can_generate_en_vector,
                        'can_generate_label_zh_vector': can_generate_label_zh_vector,
                        'can_generate_label_en_vector': can_generate_label_en_vector,
                        'descriptions_zh_len': len(descriptions_zh) if descriptions_zh else 0,
                        'descriptions_en_len': len(descriptions_en) if descriptions_en else 0,
                    })
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜
                if not label:
                    errors.append(f"ç¬¬ {line_num} è¡Œ: ç¼ºå°‘ label å­—æ®µ")
                if not can_generate_zh_vector and not can_generate_en_vector:
                    warnings.append(f"ç¬¬ {line_num} è¡Œ: æ— æ³•ç”Ÿæˆä»»ä½•å‘é‡ï¼ˆç¼ºå°‘æè¿°å’Œæ ‡ç­¾ï¼‰")
                
            except json.JSONDecodeError as e:
                invalid_lines += 1
                errors.append(f"ç¬¬ {line_num} è¡Œ: JSON è§£æå¤±è´¥ - {str(e)[:100]}")
            except Exception as e:
                invalid_lines += 1
                errors.append(f"ç¬¬ {line_num} è¡Œ: å¤„ç†å¤±è´¥ - {str(e)[:100]}")
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
    print(f"  æ€»è¡Œæ•°: {total_lines}")
    print(f"  æœ‰æ•ˆè¡Œæ•°: {valid_lines}")
    print(f"  æ— æ•ˆè¡Œæ•°: {invalid_lines}")
    
    print(f"\nğŸ“‹ å­—æ®µç»Ÿè®¡ï¼ˆå‡ºç°æ¬¡æ•°ï¼‰:")
    for field, count in sorted(field_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / valid_lines * 100) if valid_lines > 0 else 0
        print(f"  {field}: {count} ({percentage:.1f}%)")
    
    if missing_fields:
        print(f"\nâš ï¸  ç¼ºå¤±å­—æ®µç»Ÿè®¡:")
        for field, count in sorted(missing_fields.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / valid_lines * 100) if valid_lines > 0 else 0
            print(f"  {field}: {count} ({percentage:.1f}%)")
    
    # åˆ†ææ ·æœ¬æ–‡æ¡£
    if sample_docs:
        print(f"\nğŸ“ æ ·æœ¬æ–‡æ¡£åˆ†æï¼ˆå‰ {len(sample_docs)} ä¸ªï¼‰:")
        
        zh_vector_count = sum(1 for d in sample_docs if d['can_generate_zh_vector'])
        en_vector_count = sum(1 for d in sample_docs if d['can_generate_en_vector'])
        label_zh_vector_count = sum(1 for d in sample_docs if d['can_generate_label_zh_vector'])
        label_en_vector_count = sum(1 for d in sample_docs if d['can_generate_label_en_vector'])
        
        print(f"  å¯ç”Ÿæˆ descriptions_zh_vector: {zh_vector_count}/{len(sample_docs)} ({zh_vector_count/len(sample_docs)*100:.1f}%)")
        print(f"  å¯ç”Ÿæˆ descriptions_en_vector: {en_vector_count}/{len(sample_docs)} ({en_vector_count/len(sample_docs)*100:.1f}%)")
        print(f"  å¯ç”Ÿæˆ label_zh_vector: {label_zh_vector_count}/{len(sample_docs)} ({label_zh_vector_count/len(sample_docs)*100:.1f}%)")
        print(f"  å¯ç”Ÿæˆ label_en_vector: {label_en_vector_count}/{len(sample_docs)} ({label_en_vector_count/len(sample_docs)*100:.1f}%)")
        
        # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        print(f"\n  å‰5ä¸ªæ ·æœ¬æ–‡æ¡£è¯¦æƒ…:")
        for i, doc in enumerate(sample_docs[:5], 1):
            print(f"    {i}. è¡Œ {doc['line_num']}: {doc['label'][:30] if doc['label'] else 'N/A'}")
            print(f"       æè¿°(zh): {'âœ“' if doc['has_descriptions_zh'] else 'âœ—'} ({doc['descriptions_zh_len']}å­—)")
            print(f"       æè¿°(en): {'âœ“' if doc['has_descriptions_en'] else 'âœ—'} ({doc['descriptions_en_len']}å­—)")
            print(f"       åˆ«å(zh): {'âœ“' if doc['has_aliases_zh'] else 'âœ—'}")
            print(f"       åˆ«å(en): {'âœ“' if doc['has_aliases_en'] else 'âœ—'}")
            print(f"       å¯ç”Ÿæˆå‘é‡: zh_desc={doc['can_generate_zh_vector']}, en_desc={doc['can_generate_en_vector']}, zh_label={doc['can_generate_label_zh_vector']}, en_label={doc['can_generate_label_en_vector']}")
    
    # è¾“å‡ºé”™è¯¯å’Œè­¦å‘Š
    if errors:
        print(f"\nâŒ é”™è¯¯ ({len(errors)} ä¸ªï¼Œæ˜¾ç¤ºå‰10ä¸ªï¼‰:")
        for error in errors[:10]:
            print(f"  {error}")
        if len(errors) > 10:
            print(f"  ... è¿˜æœ‰ {len(errors) - 10} ä¸ªé”™è¯¯")
    
    if warnings:
        print(f"\nâš ï¸  è­¦å‘Š ({len(warnings)} ä¸ªï¼Œæ˜¾ç¤ºå‰10ä¸ªï¼‰:")
        for warning in warnings[:10]:
            print(f"  {warning}")
        if len(warnings) > 10:
            print(f"  ... è¿˜æœ‰ {len(warnings) - 10} ä¸ªè­¦å‘Š")
    
    # æ€»ç»“
    print(f"\n{'=' * 70}")
    if invalid_lines == 0 and len(errors) == 0:
        print("âœ… æ•°æ®æ–‡ä»¶æ ¼å¼æ ¡éªŒé€šè¿‡ï¼")
        print(f"   å¯ä»¥ç”Ÿæˆå‘é‡çš„æ–‡æ¡£æ¯”ä¾‹:")
        print(f"   - descriptions_zh_vector: {zh_vector_count/len(sample_docs)*100:.1f}%")
        print(f"   - descriptions_en_vector: {en_vector_count/len(sample_docs)*100:.1f}%")
        print(f"   - label_zh_vector: {label_zh_vector_count/len(sample_docs)*100:.1f}%")
        print(f"   - label_en_vector: {label_en_vector_count/len(sample_docs)*100:.1f}%")
        return True
    else:
        print("âŒ æ•°æ®æ–‡ä»¶æ ¼å¼æœ‰é—®é¢˜ï¼Œè¯·å…ˆä¿®å¤åå†å¯¼å…¥")
        return False
    print(f"{'=' * 70}\n")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # é»˜è®¤æ£€æŸ¥çš„æ•°æ®æ–‡ä»¶
    data_files = [
        "zh_wiki_v2.jsonl",
        "en_wiki_v3.jsonl"
    ]
    
    # å¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨å‚æ•°æŒ‡å®šçš„æ–‡ä»¶
    if len(sys.argv) > 1:
        data_files = sys.argv[1:]
    
    print("=" * 70)
    print("æ•°æ®æ–‡ä»¶æ ¼å¼æ ¡éªŒå·¥å…·")
    print("=" * 70)
    print()
    
    all_valid = True
    for file_path in data_files:
        if os.path.exists(file_path):
            valid = validate_jsonl_file(file_path)
            if not valid:
                all_valid = False
            print()
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\n")
    
    if all_valid:
        print("âœ… æ‰€æœ‰æ•°æ®æ–‡ä»¶æ ¡éªŒé€šè¿‡ï¼Œå¯ä»¥å¼€å§‹å¯¼å…¥ï¼")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æ•°æ®æ–‡ä»¶æœ‰é—®é¢˜ï¼Œè¯·å…ˆä¿®å¤åå†å¯¼å…¥")
        return 1


if __name__ == "__main__":
    exit(main())
